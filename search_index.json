[["index.html", "PO3B3: Quantitative Pathway Preface", " PO3B3: Quantitative Pathway Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 05 December, 2024 Preface PO3B3 – as it is coded now – has undergone quite a few iterations in the last few years, and recently the department has made the decision to remove the quantitative component from it, in order to make the teaching in quantitative methods more resilient (I am teaching three other QM modules, and so hell breaks loose when I fall ill). I do find, however, that some students are disappointed by this, as they selected the module to brush up – or acquire – statistical skills. If this is you, then these pages provide all of the material I provided in the previous version of this module (PO33Q) which should bring students without a quantitative background up to conducting a time-series probability model. Please note that I will not actively teach this pathway on the module, and that as such, and therefore you adopt this at your own risk. Having said that, I am very happy to support you (A&amp;F, seminars, etc.) if you choose to do so. I will also indicate at the end of each lecture in the “Homework” Section which tasks I am expecting you to complete should you adopt this quantitative pathway. "],["introduction-to-r.html", "1 Introduction to R 1.1 How Data are organised 1.2 R &amp; RStudio – Installation 1.3 R - Getting Started 1.4 RScript 1.5 First Steps in R 1.6 The Working Directory 1.7 R Packages 1.8 Working with Your Data Set 1.9 Data Manipulation 1.10 The Real Data Set", " 1 Introduction to R 1.1 How Data are organised I have put a little sample data set together for you which you can see here: Figure 1.1: Sample Data Set Each column represents a variable, the first one country names, and the second per capita GDP in 2015. Each row represents an observation, in our case an individuakl country. The meeting point between the the variable and the observation is a particular value. So for example, in Figure 1.1 the GDP (second column) of the third country (row) is $ 5792 (cell). You see in the first row the names of the variables. Always make these short and sweet, but especially telling. Don’t go for something like “x4_st”, or “fubar”, as nobody (including yourself after a little while) will have any clue what this is. 1.2 R &amp; RStudio – Installation Now we are ready to start working with R. The first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio - you do not need to open R itself, as we will be operating it through RStudio. 1.3 R - Getting Started In this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. Let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 1.2: RStudio It has – for now – three components to it. On the left hand-side you see the so-called into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the which consists of an upper and a lower window. The upper window has three tabs in it. The tab will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working dorectory here which gives you a good overview of the files you need to refer to for a particular project. The tab will display the graphs we will be producing. form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab . 1.4 RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the . You can, but you shouldn’t. What you should be using instead is an . An is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” (King, 1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an , click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 1.3: The RScript Window You can now write your commands in the , where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. Figure 1.4 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Preferences \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Vibrant Ink. Figure 1.4: Example of an RScript If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. 1.4.1 RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an . But apart from genetical dispositions, a well-organised is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an , for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). 1.5 First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type 5+3 and press “enter”. In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a hash tag. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superflous really, but we will soon encounter situations in which results can have a number of different items. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called , for example, we call1 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 1.6 The Working Directory It makes sense to create a folder in the file system on your computer for each and every R project. You can then think of this folder as a drawer from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). We call such a folder a . Create a new folder now that will serve as working directory for this seminar and pop the file “example.xlsx” into it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO3B3/Worksheets/WEEK 1&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. 1.7 R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called . You can install any package with the command where the package name goes, wrapped in quatation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the command. library(readxl) Once you close R at the end of a session, the library will be reset and you have to load packages from scratch (no need to install them again, though). 1.8 Working with Your Data Set 1.8.1 Opening We are now ready to open the data set in R - where it is called a “data frame”. For this, we create a new object , and ask R to read “Sheet 1”” of the Excel file “example.xlsx”. example &lt;- read_excel(&quot;files/Week 1/example.xlsx&quot;, sheet=&quot;Sheet1&quot;) We can now use our data in R! 1.8.2 Viewing the Data In the present case, you know what the data look like, but very often when you use secondary data sets, you don’t. So it’s a good idea to view the data frame before doing anything with it. You can use the command which will open a new window on your computer in which you can also edit values. fix(example) If you want to stay within the confines of R, then you can use the command. View(example) If you only want to see the first 6 observations of each variable, use the command: head(example) # A tibble: 6 × 2 country gdp &lt;chr&gt; &lt;dbl&gt; 1 China 13571 2 Germany 44187 3 India 5792 4 UK 38865 5 US 52704 6 Zambia 3602 If you simply want to know the variable names in the data frame, type: names(example) [1] &quot;country&quot; &quot;gdp&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the command, where “str” stands for structure: str(example) tibble [6 × 2] (S3: tbl_df/tbl/data.frame) $ country: chr [1:6] &quot;China&quot; &quot;Germany&quot; &quot;India&quot; &quot;UK&quot; ... $ gdp : num [1:6] 13571 44187 5792 38865 52704 ... 1.8.3 Variable Types in R You have seen in the output of the command that R distinguishes between a number of different variable types. Here is a broad overview of the variable types, so that you know which descriptive statistics you can calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: – numbers (also called string) – letters Within we can distinguish between the following: - nominal - ordinal - numeric, but only “whole” numbers (discrete) - any number (interval or ratio) 1.8.4 Descriptive Statistics Quite a large number of descriptive statistics can be calculated. These are: They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. If we want to refer to a particular column in R (which is equivalent to a variable), then we need to specify the data frame within which the variable is located, followed by a $ sign and then the variable name. Schematically, this would look be written as . With this information to hand, we can calculate the mean of the variable : mean(example$gdp) [1] 26453.5 Then the median: median(example$gdp) [1] 26218 You can get information on the quartiles (remember that the median is the second quartile), the mean, as well as the minimum and maximum through one, simple command: summary(example$gdp) Min. 1st Qu. Median Mean 3rd Qu. Max. 3602 7737 26218 26454 42856 52704 These are all measures of centrality, but centrality alone does not adequately describe a distribution. You can think of two scenarios, in both we have two people in a group and we are trying to describe their age. In group one we have one person who is 50 years old, and one who is 52 years old. Average age = 51. In the second group we have a toddler aged 2, and a very old person aged 100. Same average, but a very different distribution of age. Schematically you can see this in Figure 1.5 where two distributions have the same mean, median and mode, but their spread is quite different. Figure 1.5: Distributions with different Standard Deviations Therefore, we also need to look at the variability of a variable to adequately describe it. Again, there are quite a few measures available. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you the range straight away: min(example$gdp) [1] 3602 max(example$gdp) [1] 52704 range(example$gdp) [1] 3602 52704 The stadard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(example$gdp) [1] 21319.75 As you know, the variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(example$gdp) [1] 454531709 1.9 Data Manipulation 1.9.1 Recoding When conducting quantitative research, variables will rarely come in the format in which you require them to be. I have been kind and reshaped all data you will be using for this module already. Nonetheless, you might come into a position in which you need to recode a variable, and here is how to do it. The process is a little more involved, and requires a new package to be installed and loaded: . This package is part of the so-called which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling . Let’s go all out with the : library(tidyverse) Now we can recode. Let’s say we want to create a new variable with two categories: low income and high income, where the cut-off sits at $ 20,000. The comnand to do this takes a little explaining. We start by stating the dataframe we wish to work with, . The symbol which follows, , reads as “and then”, and is called (yes seriously) a pipe. So we take the data frame “and then” carry out a function called . This function in turn defines the new variable by recoding the variable . The command then specifies all categories of the “old” variable and what their respective values in the “new” variable are going to be. The categories in each are set in quotation marks, as they are factor / character categories. The last step is then to assign this newly created variable to our data frame . example %&gt;% mutate(gdpcat= ordered( cut(gdp, breaks=c(0, 20000, Inf), labels=c(&quot;low&quot;,&quot;high&quot;)))) -&gt; example Let us now check the structure of the new variable to make sure that we have done everything correctly. str(example$gdpcat) Ord.factor w/ 2 levels &quot;low&quot;&lt;&quot;high&quot;: 1 2 1 2 2 1 In my case all looks fine - make sure yours looks the same. 1.9.2 Saving Please save the into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the worksheet or the data, as running the on the raw data will bring you precisely to where you left off. 1.10 The Real Data Set Download the data set from the Downloads Section and place it into the current working directory. world &lt;- read.csv(&quot;files/Week 1/world.csv&quot;) Again, let’s explore the data set through the command: view(world) Now, don’t panic. This data set is big, but you know the basic structure: country by year in rows, and the variables in the column. The value for the variable in a given country in a given year is in the meeting point between row and column. The data are (yes, the word “data” is plural) organised by country name in the first instance. So the first country you see is Afghanistan. There are multiple rows for Afghanistan, because each row gives you information about the value in a particular year (second column). This is repeated for every country in the world, leading to a total of 7,666 observations, to save you scrolling all the way down. Schematically, the structure of this data set looks as follows: As you can see, not every box contains a value - the data are missing for this particular country-year for that particular variable. This is an issue we will be discussing a later stage in more detail, but I can already say now, that this issue is more pronounced in developing countries than in developed ones, and that it often severely limits the variables you can include in the analysis. Let us conclude today with some more descriptive statistics to get used to entering and executing commands. Say, we want to find out the average GDP per capita of the UK since 1960 (which is when this data set begins, it ends in 2015). To do this, we first need to create a data frame which only contains data for GB. We call this process “subsetting”. The function comes out the package, and takes a particular data frame and filters all those observations which meet the condition specified. We will use this a lot on this module, and so it makes sense to remember this one well: GB &lt;- filter(world, countrycode==&quot;GBR&quot;) We can now produce summary(GB$gdppc) Min. 1st Qu. Median Mean 3rd Qu. Max. 1398 3606 14553 18629 29091 50398 and all other descriptive statistics outlined before for “Great Britain”. If you want the number of observations, then you can display them by calling: length(GB$gdppc) [1] 56 In this particular case, it would tell us that we have data for 56 years. If this all seems a little much at the moment, don’t worry. As we go through the module, R will become much, much easier to handle! 1.10.1 Data Set – Codebook You can download a full codebook here. To “call” means to execute a command.↩︎ "],["linear-regression---theory.html", "2 Linear Regression - Theory 2.1 Introduction 2.2 What is it? 2.3 Ordinary Least Squares (OLS)", " 2 Linear Regression - Theory 2.1 Introduction Regression is the power house of the social sciences. It is widely applied and takes many different forms. In this Chapter we are going to explore the linear variant, also called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In the following Chapter we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression. 2.2 What is it? Regression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the effect. Let us choose Y as our dependent variable, and X as our independent variable. We have some data which we are displaying in a scatter plot: With a little goodwill we can already see that there is a positive relationship: as X increases, Y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line. The position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much Y increases (or decreases is the slope is negative) if we add an additional unit of X. In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients. You can see this depicted graphically here: In the context of the module, we would for example have per capita GDP on the x-axis (independent variable), and Polity V measuring the level of democracy on the y-axis (dependent variable). This would look like this in the year 2015: Figure 2.1: Democracy and Development in 2015 Population We will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows: \\[\\begin{equation} E(Y|X_{i}) = \\beta_{0} + \\beta_{1} X_{i} \\end{equation}\\] Because wer are dealing with the population, the line is the geometric locus of all the expected values of the dependent variable Y, given the values of the independent variables X. This has to do with the approach to statistics that underpins this module: frequentist statsctics (as opposed to Bayesian statistics). We are understanding all values to be “in the long run”, and if we sampled repeatedly from a population, then the expected value is the value we would, well, expect to see most often in the long run. The regression line is not intercepting with all observations. Only two are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(Y|X_{i})\\) and \\(Y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\). To describe the observations \\(Y_{i}\\) we therefore need to add the error terms to the PRF: \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i} \\end{equation}\\] Sample In reality we hardly ever have the population in the social sciences, and we generally have to contend with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive fromt he sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF. In order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this: \\[\\begin{equation} \\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} \\end{equation}\\] Analogously, we would would describe the observations \\(Y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation. \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} + \\hat{\\epsilon}_{i} \\end{equation}\\] The following graph visdualised the relationship between an observation, the PRF, the SRF and the respective error terms. 2.3 Ordinary Least Squares (OLS) When you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error term \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero. But similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS). We are now ready to apply this stuff to a PO3B3-related example! "],["linear-regression-application.html", "3 Linear Regression – Application 3.1 The Basic Command 3.2 Interpreting the Output 3.3 Choosing Variables", " 3 Linear Regression – Application 3.1 The Basic Command The command to run a regression in R is beguilingly simple: regression &lt;- lm(devar ~ indepvar, data=dataframe) We specify an object into which we store the results of the regression, here , and assign a function to this object called which stands for “linear model”. It is then convention to state the dependent variable first in the command, which in our case will always be democracy. This is then followed by a tilde and the independent variable which you want to include. In the case of modernisation theory, you might want to test what influence per capita GDP has on democracy. This week we are using the Polity IV scale to measure democracy. “The ‘Polity Score’ captures [the] regime authority spectrum on a 21-pont scale ranging from -10 (hereditary monarchy) to +10 (consolidated democracy). (…) The Polity scheme consists of six component measures that record key qualities of of executive recruitment, constraints on executive authority and political competition. It also records changes in the institutionalized qualities of governing authority.”2 We start by setting the working directory setwd(&quot;~/Warwick/Modules/PO3B3//Week 2&quot;) and then load the data set into the workspace. We subset this data frame to observations from the year 2000, only. europe &lt;- read.csv(&quot;files/Week 2/Europe.csv&quot;) library(tidyverse) europe_2000 &lt;- filter(europe, year == &quot;2000&quot;) Our dependent variable is called . The independent variable is called . We are now ready to run our first regression: reg_pol &lt;- lm(polity ~ gdppc, data=europe_2000) We can then produce a summary of the results as follows: summary(reg_pol) Call: lm(formula = polity ~ gdppc, data = europe_2000) Residuals: Min 1Q Median 3Q Max -14.1330 -0.6741 0.4742 1.4750 2.6515 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.997e+00 7.859e-01 8.902 8.61e-10 *** gdppc 1.068e-04 4.055e-05 2.635 0.0134 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.973 on 29 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.1932, Adjusted R-squared: 0.1654 F-statistic: 6.944 on 1 and 29 DF, p-value: 0.01336 We can extract the number of observations used for the estimation by calling: nobs(reg_pol) [1] 31 There is a lot of information in this, and I will take you through the output step by step now. 3.2 Interpreting the Output 3.2.1 The Number of Observations nobs(reg_pol) [1] 31 Letl’s deal with the last step first. The number of observations is equal to the number of countries in this case. We have subset the data to the year 2000, and so we have 33 countries in the analysis. This seems trivial for now, but it will become important later on. Once observations are missing, R drops them from the analysis – especially in developing countries where data are often missing in large quantities this can lead to a rapid decimation in the number of observations. This in turn is problematic for the strength inference we can draw from the analysis. 3.2.2 The Intercept R always shows the value for the intercept in the intuitively labelled row “(Intercept)” and the column “Estimate”. In this case the value is 7.0. What does this coefficient mean, substantively? When you remember the graph depicting the regression line, this is the point where the line intercepts the y-axis. So, it is the value of \\(y\\), here democracy in the form of the Polity V score, when \\(x\\), here economic development in the form of GDP, is zero. So in other words, a country with a GDP per capita of zero would achieve a Polity IV score of 7. The substantive interpretation sometimes makes sense (like here), but sometimes cannot be interpreted in this way. 3.2.3 The Slope Coefficient The slope coefficient is shown in the row depicting the name of the independent variable, here “gdp”, and again the column “Estimate”. Our slope here, is \\(1.0681e-4\\). The \\(e-4\\) means that we have to move the decimal point 4 units to the left, so written fully, this means \\(0.0001068\\). We interpret it as follows: for every additional unit of per capita GDP, measured in US$, the Polity V score increases by \\(0.0001068\\), on average. This seems very small, but when you consider the size of GDP per capita in many countries, it seems logical that this value is as small as it is. If the coefficient was negative, then this would mean that for for every additional unit of per capita GDP, measured in US$, the Polity V score would decrease by \\(0.0001068\\), on average. 3.2.4 The all-important p-value in Regression Everybody is obsessed with the p-value in quantitative research, but what does it mean in this context and where can you find it? You will have watched the video on the p-value more generally. If you haven’t, or have forgotten what it says, watch it now before reading on. First things first: how do we interpret the p-value for regression? We do a regression in order to ascertain whether there is a relationship between the independent and the dependent variable, or not. For testing whether there is one, we start from the assumption that there is none. This is what we call the null hypothesis; in our example here it would be that per capita GDP does not influence the level of democracy in a country. Now, remind yourself of the normal distribution from the video which has the mean age in its centre. In the video we were interested whether age influences height. Assuming, that there is no relationship between age and height, a regression line would look as follows: The line is perfectly flat (the slope coefficient is zero), intercepting the y-axis at the mean. This means, that for every age we expect the same height, so on average we are always right. If we now put a zero slope coefficient, such as the one for height, just in the more general form of \\(\\beta_{2}\\) in the centre of a normal distribution, it looks like this: What we want to test now, with regression, is whether the slope coefficient R has calculated for us (let me denote the estimated value of \\(\\beta_{2}\\) as \\(\\hat{\\beta_{2}}\\)), is far enough from this mean of zero, to say that we can be sure to say that there is a relationship. The statement that there is a relationship between the independent and the dependent variable, is called the alternative hypothesis. In our case the alternative hypothesis would read: “The level of per capita GDP influences the level of democracy in a country”. So how far away from the centre of zero do we have to go to say that there is indeed relationship, or put differently, that our alternative hypothesis is true? The standard in political science is that we need to have a 5% probability of finding a value more extreme than the one we have observed. Under the curve in the following graph, that is equal to the blue area on the right. And that is the p-value. If this area is 5% or less, then we have observed a value for the slope coefficient which is so far away from our assumed mean of zero, that we have sufficient evidence to reject the null hypothesis, and to accept the alternative hypothesis. But now, you might say, a slope coefficient can also be negative – here we are only looking at the right hand-side, and therefore at the scenario in which a slope coefficient is positive. And you are right. The scatter plot could give us a negative line. So, if we want to move away far enough from the assumed mean of zero in the centre, we must do so in both directions, to the left and to the right. Now, we need a value that is so far out, that to either side of the distribution, 2.5% of the area are left under the curve (2.5% on the left plus 2.5% on the right make the overall 5% we are interested in). We call this a two-sided test, whereas the scenario above is a one-sided test. The p-values reported by R for the slope coefficients are always two-sided tests (unless we tell R not to, but we are not doing that on this module). This value gives us the area under the normal distribution to the left and the right beyond our observed value, as shown in this figure: The value we are looking at in R to determine the p-value, is in the column \\(Pr(&gt;|t|)\\). In order to satisfy the requirement of the p-value being 5% or less, this value needs to be smaller than 0.05. Otherwise, more than 5% area are left, and we are not certain enough that our value is far enough away from the zero mean in the centre to say that it is “statistically different” from it. When we look at the value for the slope coefficient gdppc here, \\(0.0134\\), this means that the areas on the left and the right are jointly 1.34% – small enough for us to be sure to have found a value that is far enough away from zero to claim that there is a relationship. We therefore reject the null hypothesis, and accept the alternative hypothesis: we find evidence for a relationship between per capita GDP and Polity V in Europe in the year 2000. Again, if we want to visualise this, the actual p-value of \\(0.0134\\) would look like this: 3.2.5 The Goodness of Fit (R-Squared) Recall from the video that the goodness of fit is a measure to indicate how much of the variation in the dependent variable (democracy) the independent variable (per capita GDP) is explaining. For this, we take the ratio of the explained sum of squares over the total sum of squares. The resulting percentage is R-Squared. This number can also be found on the R output, and is in our case \\(0.1932\\), or 19.32%. This value is not too bad for a single variable! The maximum we can explain is of course 100% with an R-Squared value of 1.0, even though this is a dream never achieved empirically. But we are stil quite some distance of this dream, and can probably do better. There surely must be factors other than per capita GDP that explain democracy in Europe in the year 2000. 3.3 Choosing Variables 3.3.1 Can I choose more than one independent variable? Yes, you can! And this is where the fun starts, because now we are getting a step closer to the real world. New modernisation posits that democracy is multi-causal, and does not rest on the influence of GDP alone. Instead, it puts forward a number of concepts that act as independent variables, one of which is health. We can measure health through Hospital beds (per 1,000 people), and include this in our model, on top of GDP. To do this we type: reg_pol1 &lt;- lm(polity ~ gdppc + hospital, data=europe_2000) You see that adding independent variables is easy, we just add them on with a plus sign. It does not matter whether the expected direction of influence is positive or negative, always add additonal variables with a “+”. The command ought to lead to the following output: summary(reg_pol1) Call: lm(formula = polity ~ gdppc + hospital, data = europe_2000) Residuals: Min 1Q Median 3Q Max -11.0906 -0.4154 0.2112 1.4259 3.5517 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.134e+01 1.903e+00 5.959 2.05e-06 *** gdppc 7.668e-05 3.934e-05 1.949 0.0614 . hospital -5.828e-01 2.361e-01 -2.469 0.0199 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.742 on 28 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.3374, Adjusted R-squared: 0.2901 F-statistic: 7.13 on 2 and 28 DF, p-value: 0.003142 nobs(reg_pol1) [1] 31 3.3.2 Ceteris Paribus Let us focus on the coefficient for hospital beds first. Its value is rounded \\(-0.5828\\), implying that for every additional hospital bed per 1,000 people, the Polity V score decreases by \\(0.5828\\) units on average. So far, so good, but as we have included other variables in the regression model, namely per capita GDP, we need to account for this fact in our interpretation. We do this by adding “all other things being equal” (Latin: ceteris paribus) to this interpretation. What does this mean? It means, that if we take into account the level of per capita GDP, and hold this level constant, then for every additional hospita bed, the Polity IV score decreases by \\(0.5828\\) units on average. As such, we force the regression model to isolates the effect of hospital beds by including other possible explanatory factors, such as per capita GDP. You will sometimes read this in articles in the form of “controlling for”. To give you a different example: suppose we want to find out whether sex influences income. We could simply run a regression with income as the dependent variable, and sex as the independent variable. But we also know, that age influences income, as with increasing age people have more experience which is reflected in their salary. So even though we are not interested in the amount age influences income, we would include it in the regression model, so as to isolate the effect of the variable we are interested in: sex. Back to our modernisation example and life expectancy. The p-value for this coefficient is \\(0.0199\\), and therefore well below the required 5% threshold. We can conclude that the number of hospital beds influence the level of democracy in Europe in the year 2000. Per capita GDP is rendered insignificant, however. 3.3.3 Parsimony But can you just add independent variables at your leisure? The short answer is no. The long answer is: parsimony. This means “as few as possible, as many as necessary”. The “necessary” component is guided by the theoretical underpinning of your investigation. For example, you subscribe to new modernisation theory, and believe that that it is not only economic development in the form of per capita GDP that determines the level of democracy, but that indicators of social change also play an important role. Now it is your job as a researcher to decide how we measure social change. Do we include education? And if so, how do we measure it, say by literacy levels? Should we choose a different measure for health that measures it more directly than hospital beds? Then we might decide on life expectancy. But are these two enough to measure social change, or do we need to look at other facets? We seek to include as few as possible to produce an empirical picture of social change, but so many that we are doing proper justice to the theory. We can then proceed to test different scenarios. For example, does economic development already explain democracy? What happens if we add social change? Or does social change explain democracy on its own, already? These questions lead to the topic of “model specification”, which we will discuss in greater detail in week 4. 3.3.4 R-Squared Again As soon as we introduce more than one independent variable to the model, we cannot use “Multiple R-Squared” any more. The reason is that this measure cannot properly take into account added variables. It will either stay the same, or increase, it cannot decrease. This of course, makes no sense, for example if we add average shoe size in 2000 to the analysis, this would not help to explain democracy, but Multiple R-Squared would still likely go up. We therefore need to new measure, calles “Adjusted R-Squared” which not only penalises us for adding more indepenent variables, but will also decrease if a variable takes explanatory power away from a model. You find it here: For more detail see .↩︎ "],["important-disclaimer.html", "4 Important Disclaimer", " 4 Important Disclaimer I have discussed linear regression with you this week for two reasons: I am trying to sketch the methodological development through which the relationship between economic development and democracy has been analysed over time. Lipset started with a simple correlation analysis in 1953, and this quickly developed into linear regression analysis, as it is more powerful and more sophsticated than correlation analysis. It forms the foundation for the binary response models (probit in our case) which we will start with next week. To start with, we will only look at the probability to be democratic in a particular year across different countries. But in week 7, we will extend this to not only investigate this relationship across countries, but also over time. This dynamic probit, or Markov Transition Model (MTM), is the model you need to apply in the assessment of the module. Please DO NOT stick to linear regression in the assessment, as OLS is not capable to deal with time-series data. The same applies for a cross-sectional probit. The only method permissible int he assessment is an MTM. "],["probit.html", "5 Probit 5.1 Preliminaries 5.2 The Command 5.3 The Coefficients 5.4 Pedicted Probabilities 5.5 How to Report Results", " 5 Probit 5.1 Preliminaries As you know from the video, the relationship between our independent variable and the probability of democracy is not linear: the curve was s-shaped, so that the increment in probability is not the same for, say moving from \\(x_{1}\\) to \\(x_{2}\\) and from \\(x_{2}\\) to \\(x_{3}\\). For illustration see the red bars in the following Figure: What we therefore need to do in the world of probit, is to evaluate the probability at individual values of \\(x_{i}\\), or put differently, assess how much of the bell-shaped curve has slid across our cut-off point \\(\\tau\\) (tau) at a particular point \\(x_{i}\\). 5.2 The Command The command to run a probit is as follows: model &lt;- glm(depvar ~ indepvar data = dataframe, family = binomial(link = &quot;probit&quot;)) stands for “General Linear Model”. We then specify a model in the same way as last week, stating the dependent variable first, followed by a tilde and then the independent variable(s). We complete the command by naming the data frame we wish to use and selecting the model type, in our case it is a probit model which is binomial (our dependent variable only has two possible outcomes). Once again, let us do a specific example with the data set “Europe” in the year 2000. First set the working directory for today’s seminar: setwd(&quot;~/Warwick/Modules/PO3B3/Week 3&quot;) and then load the data set into a data frame called which we subset to the year 2000. library(haven) europe &lt;- read_dta(&quot;files/Week 3/Europe.dta&quot;) library(tidyverse) europe2000 &lt;- filter(europe, year==2000) europe2000$lifeexp &lt;- as.numeric(as.character(europe2000$lifeexp)) For a probit regression, we cannot use the Polity IV scale any more, because it is continuous, and not binary. We therefore switch the democracy coding to the one used by Przeworski et al. (2000) which you are (should be, anyway) familiar with from the substantive reading for today. We will use life expectancy as our independent variable, and examine the situation in the year 2000. To do this, we call: probit &lt;- glm(democracy ~ lifeexp, data = europe2000, family = binomial(link = &quot;probit&quot;)) This should lead to the following results: summary(probit) Call: glm(formula = democracy ~ lifeexp, family = binomial(link = &quot;probit&quot;), data = europe2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -13.45897 6.92176 -1.944 0.0518 . lifeexp 0.20357 0.09658 2.108 0.0350 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 20.824 on 36 degrees of freedom Residual deviance: 14.800 on 35 degrees of freedom (1 observation deleted due to missingness) AIC: 18.8 Number of Fisher Scoring iterations: 7 5.3 The Coefficients The coefficients are displayed in the results as follows: But how do we interpret them? As mentioned at the start of this worksheet, we cannot interpret either in a straightforward way, as the relationship between the independent variable and the probability outcome is not linear. Instead, we are going to use predicted probabilities, which evaluate the cdf for those values of the independent variable we are interested in. 5.4 Pedicted Probabilities Once we have estimated the model, we have determined the shape of the s-shaped curve at the start of the worksheet. What we now need to do, is to evaluate the probability on the y-axis for different values on the x-axis. In our case this is the variable . 5.4.1 Setting the x-values Let us first get a basic overview of the variable . We can do this by calling summary(europe2000$lifeexp) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 65.48 72.81 76.54 75.11 77.99 79.78 1 So we know that the average life expectancy in Europe in the year 2000 was 75 years,with a minimum of 65.5 years, and a maximum of 80 years. We also have one missing observation () which we need to exclude from the following by setting 3, as R is otherwise unable to calculate descriptive measures within the function, such as the mean. Let us set to its mean now, by typing: setx = data.frame(lifeexp=75.11) If you want to change this value for calculations later on, you simply set different values in the dataframe )or specify a new one). We now have the shape of the probability curve, and we have agreed on a point on the x-axis. We are finally ready to have a look at the probability to be a democracy at this point. 5.4.2 Predicting the Probability The quantity of interest we are interested in is the probability of being a democracy. To calculate this quantity of interest, we use the function and specify within this functon for which model we want to calculate the quantity, and the data frame in which we specified the value of our x-variable. In our case this variable is and we have assigned the mean value to an object called earlier. predict(probit, setx, type=&quot;response&quot;) 1 0.9664841 Remember that we have put the value of life expectancy at the mean. For this level of life expectancy R returns to us a probability of being a democracy at 96.64%. Contrarily, this means that probability to be an autocracy is 3.35% (the two probabilities always sum up to 1). Now we set life expectancy to its minimum setx = data.frame(lifeexp=min(europe2000$lifeexp, na.rm = T)) and calculate the quantity of interest again. Our probabilities have changed very drastically (ensure you get the same results before proceeding): predict(probit, setx, type=&quot;response&quot;) 1 0.4489912 If we set life expectancy to its maximum (how?), then we receive the following regime probabilities: predict(probit, setx, type=&quot;response&quot;) 1 0.9972967 Now we can make statements such as: In 2000, a European country’s probability to be a democracy with average life expectancy was 96.64%. At the minimum life expectancy of 65.5 years, this probability drops by 51.75% to 44.89%. As such, a European country in 2000 at minimum life expectancy would more likely be an autocracy than a democracy (why?). 5.5 How to Report Results The question is now: how do you report all of this in an article, or closer to home, in your assessment? Let us start by calculating a model which assesses the impact of per capita GDP on the probability to be a democracy in 2000, worldwide. The R commands and output look like this: world &lt;- read_dta(&quot;world.dta&quot;) world2000 &lt;- filter(world, year==2000) world2000$gdp_pc &lt;- as.numeric(as.character(world2000$gdp_pc)) probit &lt;- glm(democracy ~ gdp_pc, data = world2000, family = binomial(link = &quot;probit&quot;)) summary(probit) Call: glm(formula = democracy ~ gdp_pc, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.825e-02 1.160e-01 -0.847 0.396817 gdp_pc 4.758e-05 1.272e-05 3.742 0.000183 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 242.95 on 177 degrees of freedom Residual deviance: 223.39 on 176 degrees of freedom (5 observations deleted due to missingness) AIC: 227.39 Number of Fisher Scoring iterations: 5 Now, please, please never, ever copy this into an article or an assessment, as every time you do this, a little part of me dies. Make the effort of reporting the results in a nice and neat Table, that only contains all relevant information, communicates it in an accessible form, and is clearly labelled. The output above, processed properly, would look like this: Dependent variable: Democracy per capita GDP 0.00005*** (0.00001) Constant -0.098 (0.116) Observations 178 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 You see, this table manages to convey clearly the relationship we are assessing, the label of the independent variable (not in the form of cryptic variable names), the value of the intercept, the value of the slope coefficient, their respective p-value, and the number of observations. As such, the table itself could already communicate the main take-away message without somebody looking at the text. This needs to be the goal: the table, or figure needs to be able to communicate the message without having to look at the text. In turn, the text needs to be able to communicate the message on its own without looking at the table. But both need to say the same thing. After you have reported the results of the regression output in this way, you can then proceed to interpret (in the text) the transition probabilities as discussed above. Again, you might wish to sum up the main results in tabular format. 5.5.1 Multiple Independent Variables As in multiple linear regression, you can also have multiple independent variables in a probit model. Schematically this would look as follows: probit &lt;- glm(depvar ~ indepvar1 + indepvar2 + indepvar3, data = dataframe, family = binomial(link = &quot;probit&quot;), The fun starts when you begin to calculate probabilities for different values for all of these independent variables. 5.5.1.1 Setting the x-values You can set the value for each variable individually, such as in this little example: setx = data.frame(gdp_pc=min(europe2000$gdp_pc, na.rm = T), lifeexp=75.11) 5.5.2 Predicting the Probability This is very much the same as above with one independent variable. It is crucial to remember which variables you have set at which value, so that you can interpret the probability values correctly. As in linear regression, the interpretation of individual variables is ceteris paribus. It therefore makes sense, if you wish to isolate the effect of one, single variable, only to vary the values of that variable, and to leave all the other ones the same. If you varied two variables at the same time, for example, you would no doubt see a change in probability, but you could not attribute it to a single independent variable, any more. This stands for “not available remove equals true”. I will go into more detail on missing values in the lecture next week.↩︎ "],["exercises.html", "6 Exercises", " 6 Exercises Use the global data set. Estimate a model assessing the impact of per capita GDP and “Adjusted net enrolment rate, primary (% of primary school age children)” in 2000. world &lt;- read_dta(&quot;files/Week 3/world.dta&quot;) world2000 &lt;- filter(world, year==2000) world2000$gdp_pc &lt;- as.numeric(as.character(world2000$gdp_pc)) probit1 &lt;- glm(democracy ~ gdp_pc, family = binomial(link = &quot;probit&quot;), data = world2000) summary(probit1) Call: glm(formula = democracy ~ gdp_pc, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.825e-02 1.160e-01 -0.847 0.396817 gdp_pc 4.758e-05 1.272e-05 3.742 0.000183 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 242.95 on 177 degrees of freedom Residual deviance: 223.39 on 176 degrees of freedom (5 observations deleted due to missingness) AIC: 227.39 Number of Fisher Scoring iterations: 5 Design a Table to report results in MS Word. What is the probability to be a democracy evaluated at the mean for both independent variables? What is the difference in probability ceteris paribus for minimum and maximum per capita GDP? What happens if we replace net enrolment rate by life expectancy in the model? "],["consolidation.html", "7 Consolidation 7.1 The World-Value Survey", " 7 Consolidation 7.1 The World-Value Survey The World Values Survey () is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS Association and WVSA Secretariat headquartered in Vienna, Austria. The survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world?s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones. "],["exercises-1.html", "8 Exercises 8.1 Descriptives 8.2 Linear Regression 8.3 Probit", " 8 Exercises 8.1 Descriptives For all exercises, use the “WVS” data set which is available in the Downloads Section. PLease note that you need to install and load the haven package in order to read this Stata data file into R. install.packages(&quot;haven&quot;) library(haven) wvs &lt;- read_dta(&quot;wvd.dta&quot;) What is the average value for survival/self-expression values? What does this mean? Filtering through Waves, how has this average changed? Repeat these two steps with traditional/rational values. Why is the comparison of these values over time difficult? 8.2 Linear Regression For which waves do traditional–rational values in the population explain a country’s level of democracy? For which waves do survival–self-expression values in the population explain a country’s level of democracy? Identify reasons why earlier waves fail to explain the level of democracy. Assessed jointly, how much more or less democratic do survival/self-expression value and GDP growth make countries in wave 5 (2005-2009)? 8.3 Probit For which waves do traditional–rational values in the population explain a country’s probability to be a democracy? For which waves do survival–self-expression values in the population explain a country’s probability to be a democracy? Identify reasons why earlier waves fail to explain regime type. Calculate a model assessing the impact of GDP growth on the probability of democracy for countries in wave 5. How much less likely is a country to be a democracy moving from minimum GDP growth to maximum GDP growth in wave 5? Calculate a model assessing the impact of traditional–rational values, and GDP growth on the probability of democracy for countries in wave 5. Interpret the results. Considering the main propositions of cultural modernisation, what are the implications of these results? Assess the results of each of the previous tasks in turn. You can find the solutions to these exercises in the Downloads Section. "],["markov-transition-models.html", "9 Markov Transition Models 9.1 Time Series 9.2 What are Markov Transition Models? 9.3 Democratic Emergence 9.4 Democratic Survival", " 9 Markov Transition Models 9.1 Time Series 9.2 What are Markov Transition Models? 9.2.1 Conditional Probabilities Emergence \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation}\\] Survival \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation}\\] 9.2.2 Democratic Emergence \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation*}\\] The probability of a country to be a democracy in year \\(t\\), given that it was a dictatorship in the previous year, \\(t-1\\) 9.2.3 We therefore need a variable that gives us that information, the regime type in the previous year We will use the global data set Creating it, requires the package, as part of the tidyverse library(haven) world &lt;- read_dta(&quot;files/Week 7/world.dta&quot;) library(tidyverse) We can now create the lagged democracy value world &lt;- world %&gt;% group_by(countrykey) %&gt;% mutate(l.democracy = lag(democracy)) %&gt;% ungroup() This creates a new variable in which all values are lagged by one year. As the first observation for each country cannot be lagged, it creates as many missing values as we have countries in the data set Visualised: 9.2.4 What else? The country needs to have been a dictatorship in the previous year But we also need our independent variables It is reasonable to assume, that the regime type in year \\(t\\) depends on the state of socio-economic development in the previous year, \\(t-1\\) We therefore need to lag the values of all independent variables by one year 9.2.5 Lagging the Independent Variables Same procedure as before. For “per capita GDP” we call world &lt;- world %&gt;% group_by(countrykey) %&gt;% mutate(l.gdp_pc = lag(gdp_pc)) %&gt;% ungroup() Let’s do the same for: Life Expectancy: world &lt;- world %&gt;% group_by(countrykey) %&gt;% mutate(l.lifeexp = lag(lifeexp)) %&gt;% ungroup() and Primary Gross Enrolment Rate: world &lt;- world %&gt;% group_by(countrykey) %&gt;% mutate(l.enrl_gross = lag(enrl_gross)) %&gt;% ungroup() 9.2.6 Subsetting the Data for Conditional Probabilities Select observations in which world_democ0 &lt;- filter(world, l.democracy==0) Select observations in which world_democ1 &lt;- filter(world, l.democracy==1) Recall: \\(P(y_{i,t} = 1 | y_{i, t-1} = 0)\\) Suppose, we want to see the effect of on democratic emergence. 9.2.7 Now we are ready! emergence &lt;- glm(democracy ~ l.gdp_pc, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) 9.3 Democratic Emergence summary(emergence) Call: glm(formula = democracy ~ l.gdp_pc, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.918e+00 5.247e-02 -36.551 &lt;2e-16 *** l.gdp_pc -3.845e-05 2.018e-05 -1.906 0.0567 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 773.12 on 3404 degrees of freedom Residual deviance: 767.25 on 3403 degrees of freedom (771 observations deleted due to missingness) AIC: 771.25 Number of Fisher Scoring iterations: 8 9.3.1 Full Modernisation emergence_full &lt;- glm(democracy ~ l.gdp_pc + l.lifeexp + l.enrl_gross, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) 9.3.2 Democratic Emergence – Full Modernisation summary(emergence_full) Call: glm(formula = democracy ~ l.gdp_pc + l.lifeexp + l.enrl_gross, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.255e+00 3.815e-01 -8.531 &lt; 2e-16 *** l.gdp_pc -8.832e-05 3.521e-05 -2.508 0.012131 * l.lifeexp 2.624e-02 7.870e-03 3.334 0.000855 *** l.enrl_gross -1.304e-03 2.651e-03 -0.492 0.622828 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 608.53 on 2472 degrees of freedom Residual deviance: 589.74 on 2469 degrees of freedom (1703 observations deleted due to missingness) AIC: 597.74 Number of Fisher Scoring iterations: 9 9.4 Democratic Survival 9.4.1 Conditional Probability \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation*}\\] The probability of a country to be a democracy in year \\(t\\), given that it was a democracy in the previous year, \\(t-1\\) 9.4.2 In R This is easy now, let’s do democratic survival for survival &lt;- glm(democracy ~ l.gdp_pc, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(survival) Call: glm(formula = democracy ~ l.gdp_pc, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.439e+00 1.006e-01 14.308 &lt; 2e-16 *** l.gdp_pc 4.401e-04 8.903e-05 4.943 7.69e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 503.18 on 3084 degrees of freedom Residual deviance: 416.42 on 3083 degrees of freedom (138 observations deleted due to missingness) AIC: 420.42 Number of Fisher Scoring iterations: 11 9.4.3 And Again: Full Modernisation survival_full &lt;- glm(democracy ~ l.gdp_pc + l.lifeexp + l.enrl_gross, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(survival_full) Call: glm(formula = democracy ~ l.gdp_pc + l.lifeexp + l.enrl_gross, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.397e-02 7.310e-01 -0.019 0.9847 l.gdp_pc 1.742e-04 8.695e-05 2.003 0.0451 * l.lifeexp 2.924e-02 1.370e-02 2.134 0.0328 * l.enrl_gross -7.503e-04 4.106e-03 -0.183 0.8550 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 319.90 on 2296 degrees of freedom Residual deviance: 264.38 on 2293 degrees of freedom (926 observations deleted due to missingness) AIC: 272.38 Number of Fisher Scoring iterations: 12 "],["model-fit-in-binary-response-models.html", "10 Model Fit in Binary Response Models 10.1 How to Measure Model Fit 10.2 ROC-Curves", " 10 Model Fit in Binary Response Models 10.1 How to Measure Model Fit Simple: The proportion of correctly predicted regimes But: 10.2 ROC-Curves Measure how well a model is able to separate cases Capture the trade-off between correctly predicting 1s and 0s. Construct a curve which has: y-axis: Probability of correctly predicting a 1. (Sensitivity) x-axis: (1-Specificity); Specificity: Probability of correctly predicting a 0. Diagonal: Model with no covariates 10.2.1 Perfect Separation 10.2.2 Overlap 10.2.3 Example This is for the full model of democratic survival. 10.2.4 How is this constructed? 10.2.5 Interpretation The further away from the diagonal, the better the model predicts 1s and 0s. Area: Would be 100% if the model correctly predicted everything This becomes smaller as the model becomes worse Good for comparisons between models. 10.2.6 R Let’s start with the simple emergence model Load the package library(pROC) and the calculate the curve: prob_em &lt;- predict(emergence, type=&quot;response&quot;) world_democ0$prob_em &lt;- unlist(prob_em) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em) auc(roc) Area under the curve: 0.5274 10.2.7 We can also look at this graphically again: 10.2.8 Emergence: Full Model prob_em_full &lt;- predict(emergence_full, type=&quot;response&quot;) world_democ0$prob_em_full &lt;- unlist(prob_em_full) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em_full) auc(roc) Area under the curve: 0.6413 10.2.9 Graphically 10.2.10 Survival prob_sur &lt;- predict(survival, type=&quot;response&quot;) world_democ1$prob_sur &lt;- unlist(prob_sur) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur) auc(roc) Area under the curve: 0.8504 10.2.11 For completeness’s sake This is the code for the full survival model shown at the beginning of this section: prob_sur_full &lt;- predict(survival_full, type=&quot;response&quot;) world_democ1$prob_sur_full &lt;- unlist(prob_sur_full) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur_full) auc(roc) plot(roc, print.auc=TRUE) "],["joint-estimation-of-emergence-and-survival.html", "11 Joint Estimation of Emergence and Survival 11.1 Rationale 11.2 Example 11.3 Application in R", " 11 Joint Estimation of Emergence and Survival 11.1 Rationale It is perfectly adequate to estimate the processes of democratic emergence and survival separately. There is a more elegant approach, however, in which emergence and survival are estimated together. The rationale here is that the model will, overall, use more observations and therefore this might influence standard errors and thus statistical significance. As an approach, I would estimate the joint model as outlined here to check statistical significance of variables, and then estimate emergence and survival separately for the calculation of quantities of interest (transitional probabilities). So, what’s the setup? Again, we are incorporating the lagged value \\(y_{t-1}\\), to encapsulate all the history prior to period \\(t\\). There is just one little trick I need to introduce before we can start on the model: I will replace \\(y_{t-1}\\) by an indicator variable \\(I_{D}\\) in Equation \\(\\ref{eq:interactionmodel}\\) which assumes the value 1 if a country was a democracy in the previous year, and zero otherwise (notation adapted from Epstein et al., 2006, p. 553). Following on from this, we can write the model as follows4: \\[\\begin{equation}\\label{eq:interactionmodel} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t} + \\epsilon_{i,t}) \\end{equation}\\] where \\(P(D_{it})\\) is the probability that a country \\(i\\) was a democracy in year \\(t\\), \\(\\Phi(\\text{\\textperiodcentered})\\) is the cumulative normal distribution (the s-shaped distribution which you know from the introduction of probit in week 3), \\(I_{D}\\) the aforementioned indicator variable, \\(X_{i,t}\\) is an independent variable for country \\(i\\) in year \\(t\\), and \\(\\epsilon_{i,t}\\) is a zero mean stochastic disturbance (I only include this for completeness’ sake here. As this is irrelevant for us here and now, I will drop this from the following discussion, not to confuse you unnecessarily). This model is a multiplicative interaction model which allow us to model the probability of a country to be a democracy, conditional on its regime type in the previous year. The indicator variable \\(I_{D}\\), equal to \\(y_{t-1}\\) captures this information. How does this work? Let us start by assuming that in year \\(t-1\\) country \\(i\\) was an autocracy. In this case, the indicator variable \\(I_{D}\\) would be equal to zero, and therefore Equation \\(\\ref{eq:interactionmodel}\\) can be re-written as \\[\\begin{equation} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t}) \\end{equation}\\] In this case, the coefficient \\(\\beta_{1}\\) would only represent the impact of variable \\(X_{i,t}\\) on the probability of an autocracy transitioning to democracy. Expressed more formally, we are dealing with conditional probabilities here, in the case of \\(\\beta_{1}\\), we would obtain the impact of variable \\(X_{i,t}\\) on a country to be a democracy in year \\(t\\), under the condition that it was an autocracy in year \\(t-1\\). It is conditional on this, because we set the indicator variable to zero before. We can construct a similar scenario for the condition, that a country was a democracy in the previous year. In this case, the indicator variable \\(I_{D}\\) would be equal to \\(1\\), and we would obtain equation \\(\\ref{eq:interactionmodel}\\) again: \\[\\begin{equation*} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t}) \\end{equation*}\\] With \\(I_{D}=1\\), this can be simplified to: \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) X_{i,t}) \\end{equation}\\] This equation illustrates very well, that the impact of variable \\(X_{i,t}\\) on the probability of democracy to remain a democracy is now made up out of the sum of the two coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\), whereas the constant is the sum of coefficients \\(\\beta_{0}\\) and \\(\\beta_{2}\\). 11.2 Example To make this more tangible, let’s look at an example. Assume we want to look at the effect of per capita GDP on democratic emergence (i.e. the transition of an autocracy to democracy), and democratic survival (i.e. the transition from democracy to democracy. In this case we would specify the model as follows: \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\end{equation}\\] The coefficient indicating the impact of per capita GDP on democratic emergence is \\(\\beta_{1}\\). As illustrated above, the indicator variable \\(I_{D}\\) is zero in this case, and the equation would be reduced to \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t}) \\end{equation}\\] For democratic survival, the coefficient indicating the impact of per capita GDP would be the sum of coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\) \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\text{per capita GDP}_{i,t}) \\end{equation}\\] 11.3 Application in R How does all of this look in R, and how do you apply this to a real-world scenario? Let’s do this using the above example of per capita GDP in the global data set. You have already created the indicator variable \\(I_{D}\\) in the form of the variable . What we need next is a variable that captures \\(I_{D} \\text{per capita GDP}_{i,t}\\), so that we can calculate \\(\\beta_{3}\\). To create this variable, type world$gdp_pc_l.democracy &lt;- world$l.democracy * world$gdp_pc Now, we are ready to estimate the model. Remember, formally, this is written as: \\[\\begin{equation*} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\end{equation*}\\] We replace this in the R command with the equivalent variables: joint &lt;- glm(democracy ~ gdp_pc + l.democracy + gdp_pc_l.democracy, data = world, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred You obtain the following output: summary(joint) Call: glm(formula = democracy ~ gdp_pc + l.democracy + gdp_pc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.924e+00 5.225e-02 -36.825 &lt; 2e-16 *** gdp_pc -3.456e-05 1.841e-05 -1.877 0.0606 . l.democracy 3.341e+00 1.131e-01 29.536 &lt; 2e-16 *** gdp_pc_l.democracy 4.673e-04 8.810e-05 5.304 1.14e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 9064.7 on 6546 degrees of freedom Residual deviance: 1190.4 on 6543 degrees of freedom (1051 observations deleted due to missingness) AIC: 1198.4 Number of Fisher Scoring iterations: 11 For democratic emergence, we can report the Intercept and the slope coefficient straight away as -1.92427 and -.0000346, respectively. The slope coefficient is insignificant, as the p-value is \\(&gt;0.05\\). This is in line with the findings from estimating emergence separately. For democratic survival, we need to take the sum of and to obtain the intercept, and and to obtain the slope coefficient. This calculation will yield the same coefficients as the two-stepped analysis. The last step is the assessment of statistical significance. For the emergence model, we can once again interpret the output straight away. For the survival scenario, however, we need to test the hypothesis that for example \\(\\beta_{0}\\) and\\(\\beta_{3}\\) are jointly different from zero. As the survival effect is a joint-venture between these two coefficients, we also need to assess their significance jointly, and not just concentrate on \\(\\beta_{3}\\). This is the mistake Przeworski et al. (2000) have made in their seminal book, and this is what is discussed on the first few pages of the article by Epstein et al. (2006). To do this, we are using a post-estimation command which is testing the aforementioned hypothesis that \\(\\beta_{0}\\) and\\(\\beta_{3}\\) are jointly different from zero, called a Wald-Test. For this we need to install and load a new package, called . This is a regression term test, where you first need to state the object within which the results are stored (here: ), and the two terms you want to test, preceded by a tilde and connected by a plus. Lastly we specify that we want the Wald Test. library(survey) regTermTest(joint, ~gdp_pc+gdp_pc_l.democracy, method=&quot;Wald&quot;) Wald test for gdp_pc gdp_pc_l.democracy in glm(formula = democracy ~ gdp_pc + l.democracy + gdp_pc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) F = 14.37295 on 2 and 6543 df: p= 5.9099e-07 With \\(p= 5.9099e-07\\) we can reject the null hypothesis, and conclude that jointly, the slope coefficients are different from zero, and as such that per capita GDP explains democratic survival. This discussion draws on Brambor et al., 2006. Please note that I am deliberately NOT lagging the independent variables (IVs) on this worksheet to keep notation as simple as possible. If you decide to run the interaction model, make sure you lag the IVs as explained on the exercise sheet and RScript for week 7.↩︎ "],["exercises-2.html", "12 Exercises 12.1 The Data Set 12.2 Basics 12.3 Advanced", " 12 Exercises 12.1 The Data Set Use the data set called prz.dta which is available in the Downloads Section. This is the data set used in Przeworski et al.’s 2000 book, Democracy and Development. Deposit this in an appropriate working directory and import the data set into a data frame called prz. We will only be looking at a few variables – democ = 1 if democracy, 0 otherwise; gdpw - GDP per worker; g = growth rate; oil = 1 if oil producer, 0 otherwise. 12.2 Basics Run a probit model where is the dependent variable and and are the independent variables. Put the results in column 1 of Table 1. What is (possibly) wrong with this approach? Interpret the coefficients on one or two of the variables. Run the same probit model as before but now include a lagged dependent variable. To create the lagged dependent variable, call: prz &lt;- prz %&gt;% group_by(country) %&gt;% mutate(l.democ = lag(democ)) %&gt;% ungroup() Put the results in column two of Table 1. What are we assuming by including a lagged dependent variable? Do you think that this is appropriate here? Now estimate a probit “transition to democracy” model i.e. how do growth, wealth and oil affect the probability that a country is a democracy this year given that it was a dictatorship last year. We are also lagging the independent variables by one year. Put the results in column 3 of Table 1. Interpret the sign of the coefficients on each independent variable. Now estimate a probit “survival of democracy” model i.e. how do (lagged) growth, wealth and oil affect the probability that a country is a democracy this year given that it was a democracy last year. Put the results in column 4 of Table 1. Interpret the sign of the coefficients on each independent variable. 12.3 Advanced For this section you will have to read through the instructions for joint estimation, provided separately Now interact all the lagged independent variables with the lagged dependent variable. Estimate a fully interactive model and include all the constitutive terms. Put the results in column 5 of Table 1. What is the relationship between these coefficients and those in the previous two columns? Is there any extra information provided by this full interaction model that was not available from the previous two models? Now consider the straight probit model, the probit model with the lagged dependent variable, and the full interaction model. Produce the ROC curve for each of these models. Interpret a point on one of these curves. What do the ROC curves tell you about the fit of these three models? You can find the solutions to these exercises in the Downloads Section. "],["downlads.html", "13 Downlads 13.1 Data Sets 13.2 R Scripts", " 13 Downlads 13.1 Data Sets Africa Europe Eastern Europe Latin America Middle East South East Asia Sub-Saharan Africa World World Values Survey (WVS) Przeworski et al. (2000) 13.2 R Scripts Week 1 Week 3 Week 5 Solutions Week 7 Solutions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
