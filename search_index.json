[["index.html", "PO3B3: Quantitative Pathway Preface", " PO3B3: Quantitative Pathway Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 23 January, 2025 Preface PO3B3 – as it is coded now – has undergone quite a few iterations in the last few years, and recently the department has made the decision to remove the quantitative component from it, in order to make the teaching in quantitative methods more resilient (I am teaching three other QM modules, and so hell breaks loose when I fall ill). I do find, however, that some students are disappointed by this, as they selected the module to brush up – or acquire – statistical skills. If this is you, then these pages provide all of the material I provided in the previous version of this module (PO33Q) which should bring students without a quantitative background up to conducting a time-series probability model. I will indicate at the end of each lecture in the “Homework” Section which tasks I am expecting you to complete should you adopt this quantitative pathway. All sources cited in the text are referenced in the List of References. But you can also download the full PO3B3 Bibliography. "],["website-features.html", "Website Features", " Website Features You will find embedded in the text four different types of boxes which serve different purposes: Some explanations that will hopefully make your work with this webpage or learning the material itself easier. A brief question which tests your understanding of the previous material. This appears when you need to be careful with your coding in R to avoid problems. "],["accessibility.html", "Accessibility", " Accessibility For those of you who prefer a dark background, like me, you can select this option from the menu at the top of the page. Click the “A” symbol, and then you can choose between “white”, “sepia”, or “night”. The companion uses the font “Cantarell”. "],["introduction-to-r.html", "Introduction to R How Data are organised R &amp; RStudio – Installation R - Getting Started RScript First Steps in R The Working Directory R Packages Working with Your Data Set Data Manipulation The Real Data Set Book Recommendations", " Introduction to R How Data are organised I have put a little sample data set together for you which you can see here: Figure 1: Sample Data Set Each column represents a variable, the first one country names, and the second per capita GDP in 2015. Each row represents an observation, in our case an individual country. The meeting point between the the variable and the observation is a particular value. So for example, in Figure 1 the GDP (second column) of the third country (row) is $ 5792 (cell). You see in the first row the names of the variables. Always make these short and sweet, but especially telling. Don’t go for something like “x4_st”, or “fubar”, as nobody (including yourself after a little while) will have any clue what this is. R &amp; RStudio – Installation Now we are ready to start working with R. The first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio. Whilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio. R - Getting Started In this companion I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. Let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 2: RStudio It has – for now – three components to it. On the left hand-side you see the so-called “Console” into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the “Workspace” which consists of an upper and a lower window. The upper window has three tabs in it. The tab “Environment” will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the “History” tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the “Connections” tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under “Files” you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working dorectory here which gives you a good overview of the files you need to refer to for a particular project. The “Plots” tab will display the graphs we will be producing. “Packages” form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a “Help” function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab “Viewer”. RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” (King, 1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 3: The RScript Window You can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. Figure 4 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight. Figure 4: Example of an RScript More Themes If you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from: install.packages( &quot;rsthemes&quot;, repos = c(gadenbuie = &#39;https://gadenbuie.r-universe.dev&#39;, getOption(&quot;repos&quot;)) ) rsthemes::install_rsthemes() You can also download Flo’s Dark Theme2 and then “add” it at the bottom of the “Appearance” menu. RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). Now you know how to write in an RScript, avoid using the “Console” to write your code, and only write in the RScript. First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type 5+3 and press “command” / “enter” (or “Ctrl” / “enter” if you are on Windows). In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a hash tag. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. Note that the result of this operation is displayed in the “Console”, even if you write this in the RScript above. You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call3 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 The Working Directory It is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take. In those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 5. Figure 5: Folder Structure You see that there is a sub-folder for each week of the module (I have only done a few for illustrative purposes), and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO3B3. R works with so-called “Working Directories”. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO3B3Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Please set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO3B3//Week 1&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the library() command. library(readxl) Once you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again. Working with Your Data Set Opening We are now ready to open the data set in R - where it is called a “data frame”. First, download the Example data set (also available in the Downloads Section) and place it into the current working directory. To load it into R, we create a new object example, and ask R to read “Sheet 1 of the Excel file”example.xlsx”. example &lt;- read_excel(&quot;Week 1/example.xlsx&quot;, sheet=&quot;Sheet1&quot;) Please do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need. We can now use our data in R! Viewing the Data In the present case, you know what the data look like, but very often when you use secondary data sets, you don’t. So it’s a good idea to view the data frame before doing anything with it. To view the data frame in a way you might be familiar with from Excel (even though you cannot edit this in the same way). apply the View() command. View(example) If you only want to see the first 6 observations of each variable, use the head() command: head(example) # A tibble: 6 × 2 country gdp &lt;chr&gt; &lt;dbl&gt; 1 China 13571 2 Germany 44187 3 India 5792 4 UK 38865 5 US 52704 6 Zambia 3602 If you simply want to know the variable names in the data frame, type: names(example) [1] &quot;country&quot; &quot;gdp&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure: str(example) tibble [6 × 2] (S3: tbl_df/tbl/data.frame) $ country: chr [1:6] &quot;China&quot; &quot;Germany&quot; &quot;India&quot; &quot;UK&quot; ... $ gdp : num [1:6] 13571 44187 5792 38865 52704 ... Variable Types in R You have seen in the output of the str() command that R distinguishes between a number of different variable types. Here is a broad overview of the variable types, so that you know which descriptive statistics you can calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: numeric – numbers character (also called string) – letters Within numeric we can distinguish between the following: factor - nominal variable ordered factor - ordinal variable integer - numeric, but only “whole” numbers (discrete) numeric - any number (interval or ratio scales) If you are unfamiliar with measurement scales, then please look these up before proceding. Descriptive Statistics Quite a large number of descriptive statistics can be calculated. For example: Mean Median Mode Range Standard Deviation Variance Again, if these don’t mean anything to you beyond having heard the term before, please look these up. They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. If we want to refer to a particular column in R (which is equivalent to a variable), then we need to specify the data frame within which the variable is located, followed by a $ sign and then the variable name. Schematically, this would look be written as dataframe$variable. With this information to hand, we can calculate the mean of the variable gdp: mean(example$gdp) [1] 26453.5 Then the median: median(example$gdp) [1] 26218 Mean, median, and also mode (the most frequently occurring value) are all measures of centrality, but centrality alone does not adequately describe a distribution. You can think of two scenarios, in both we have two people in a group and we are trying to describe their age. In group one we have one person who is 50 years old, and one who is 52 years old. Average age = 51. In the second group we have a toddler aged 2, and a very old person aged 100. Same average, but a very different distribution of age. Schematically you can see this in Figure 6 where two distributions have the same mean, median and mode, but their spread is quite different. Figure 6: Distributions with different Standard Deviations Therefore, we also need to look at the variability of a variable to adequately describe it. Again, there are quite a few measures available. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you the range straight away: min(example$gdp) [1] 3602 max(example$gdp) [1] 52704 range(example$gdp) [1] 3602 52704 The standard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(example$gdp) [1] 21319.75 The variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(example$gdp) [1] 454531709 You can get information on the quartiles (these are also measures of spread), the mean, as well as the minimum and maximum of a variable through one, simple command: summary(example$gdp) Min. 1st Qu. Median Mean 3rd Qu. Max. 3602 7737 26218 26454 42856 52704 Data Manipulation Recoding When conducting quantitative research, variables will rarely come in the format in which you require them to be. I have been kind and reshaped all data you will be using for this module already. Nonetheless, you might come into a position in which you need to recode a variable, and here is how to do it. The process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). Let’s go all out with the tidyverse: library(tidyverse) Now we can recode. Let’s say we want to create a new variable with two categories: low income and high income, where the cut-off sits at $ 20,000. The comnand to do this takes a little explaining. We start by stating the dataframe we wish to work with, example. The symbol which follows, %&gt;%, reads as “and then”, and is called (yes seriously) a pipe. So we take the data frame example “and then” carry out a function called mutate. This function in turn defines the new variable gdpcat by recoding the variable gdp. The command then specifies all categories of the “old” variable gdp and what their respective values in the “new” variable gdpcat are going to be. The categories in each are set in quotation marks, as they are factor / character categories. The last step is then to assign this newly created variable gdpcat to our data frame example. example %&gt;% mutate(gdpcat= ordered( cut(gdp, breaks=c(0, 20000, Inf), labels=c(&quot;low&quot;,&quot;high&quot;)))) -&gt; example Make a habit of adding a note underneath each of the more complex code chunks in your RScript (preceded with a #) in which you translate the code into plain English. Let us now check the structure of the new variable to make sure that we have done everything correctly. str(example$gdpcat) Ord.factor w/ 2 levels &quot;low&quot;&lt;&quot;high&quot;: 1 2 1 2 2 1 In my case all looks fine - make sure yours looks the same. Saving Please save the Rscript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the worksheet or the data, as running the RScript on the raw data will bring you precisely to where you left off. The Real Data Set Download the world.csv data set from the Downloads Section and place it into the current working directory. world &lt;- read.csv(&quot;Week 1/world.csv&quot;) The data are taken from World Bank (2024), Boix et al. (2018), and Marshall &amp; Gurr (2020). Table 1 provides a full code book, but you can also download it in pdf format here. Table 1: WDI Codebook variable label Country Name Country Name Country Code Country Code year year democracy 0 = Autocracy, 1 = Dictatorship (Boix et al., 2018) gdppc GDP per capita (constant 2010 US$) gdpgrowth Absolute growth of per capita GDP to previous year (constant 2010 US Dollars) enrl_gross School enrollment, primary (% gross) enrl_net School enrollment, primary (% net) agri Employment in agriculture (% of total employment) (modeled ILO estimate) slums Population living in slums (% of urban population) telephone Fixed telephone subscriptions (per 100 people) internet Individuals using the Internet (% of population) tax Tax revenue (% of GDP) electricity Access to electricity (% of population) mobile Mobile cellular subscriptions (per 100 people) service Services, value added (% of GDP) oil Oil rents (% of GDP) natural Total natural resources rents (% of GDP) literacy Literacy rate, adult total (% of people ages 15 and above) prim_compl Primary completion rate, total (% of relevant age group) infant Mortality rate, infant (per 1,000 live births) hosp Hospital beds (per 1,000 people) tub Incidence of tuberculosis (per 100,000 people) health_ex Current health expenditure (% of GDP) ineq Income share held by lowest 10% unemploy Unemployment, total (% of total labor force) (modeled ILO estimate) lifeexp Life expectancy at birth, total (years) urban Urban population (% of total population) polity5 Combined Polity V score Again, let’s explore the data set through the view() function: view(world) Now, don’t panic. This data set is big, but you know the basic structure: country by year in rows, and the variables in the column. The value for the variable in a given country in a given year is in the meeting point between row and column. The data are (yes, the word “data” is plural) organised by country name in the first instance. So the first country you see is Afghanistan. There are multiple rows for Afghanistan, because each row gives you information about the value in a particular year (second column). This is repeated for every country in the world, leading to a total of 9.456 observations, to save you scrolling all the way down. Schematically, the structure of this data set looks as follows: When you look at the data set in R, you will see that not every box contains a value - this means that the data are missing for this particular country-year for that particular variable. This is an issue we will be discussing a later stage in more detail, but I can already say now, that this issue is more pronounced in developing countries than in developed ones, and that it often severely limits the variables you can include in the analysis. Let us conclude today with some more descriptive statistics to get used to entering and executing commands. Say, we want to find out the average GDP per capita of the UK since 1960 (which is when this data set begins, it ends in 2015). To do this, we first need to create a data frame which only contains data for GB. We call this process “subsetting”. The filter function comes out the dplyr package, and takes a particular data frame and filters all those observations which meet the condition specified. We will use this a lot on this module, and so it makes sense to remember this one well: GB &lt;- filter(world, countrycode==&quot;GBR&quot;) We can now produce summary(GB$gdppc) Min. 1st Qu. Median Mean 3rd Qu. Max. 1398 3606 14553 18629 29091 50398 and all other descriptive statistics outlined before for “Great Britain”. If you want the number of observations, then you can display them by calling: length(GB$gdppc) [1] 56 In this particular case, it would tell us that we have data for 56 years. If this all seems a little much at the moment, don’t worry. As we go through the module, R will become much, much easier to handle! Book Recommendations If you want some more material to read up on R, then these are my recommendations: Fogarty (2023): The best applied R book on the market until my own book comes out. Stinerock (2022): Popular with students for good reasons! J. D. Long &amp; Teetor (2019): Some recipes to cook with R Please consult the List of References for full details. Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎ This is a variation of the Dracula Theme.↩︎ To “call” means to execute a command.↩︎ "],["linear-regression---theory.html", "Linear Regression - Theory Introduction What is it? Ordinary Least Squares (OLS)", " Linear Regression - Theory Introduction Regression is the power house of the social sciences. It is widely applied and takes many different forms. In this Chapter we are going to explore the linear variant, also called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In the following Chapter we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression. What is it? Regression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the effect. Let us choose Y as our dependent variable, and X as our independent variable. We have some data which we are displaying in a scatter plot: With a little goodwill we can already see that there is a positive relationship: as X increases, Y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line. The position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much Y increases (or decreases is the slope is negative) if we add an additional unit of X. In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients. You can see this depicted graphically here: In the context of the module, we would for example have per capita GDP on the x-axis (independent variable), and Polity V measuring the level of democracy on the y-axis (dependent variable). This would look like this in the year 2015: Figure 7: Democracy and Development in 2015 Population We will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows: \\[\\begin{equation} E(Y|X_{i}) = \\beta_{0} + \\beta_{1} X_{i} \\end{equation}\\] Because wer are dealing with the population, the line is the geometric locus of all the expected values of the dependent variable Y, given the values of the independent variables X. This has to do with the approach to statistics that underpins this module: frequentist statsctics (as opposed to Bayesian statistics). We are understanding all values to be “in the long run”, and if we sampled repeatedly from a population, then the expected value is the value we would, well, expect to see most often in the long run. The regression line is not intercepting with all observations. Only two are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(Y|X_{i})\\) and \\(Y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\). To describe the observations \\(Y_{i}\\) we therefore need to add the error terms to the PRF: \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i} \\end{equation}\\] Sample In reality we hardly ever have the population in the social sciences, and we generally have to contend with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive fromt he sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF. In order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this: \\[\\begin{equation} \\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} \\end{equation}\\] Analogously, we would would describe the observations \\(Y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation. \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} + \\hat{\\epsilon}_{i} \\end{equation}\\] The following graph visdualised the relationship between an observation, the PRF, the SRF and the respective error terms. Ordinary Least Squares (OLS) When you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error term \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero. But similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS). We are now ready to apply this stuff to a PO3B3-related example! "],["linear-regression-application.html", "Linear Regression – Application The Basic Command Interpreting the Output Choosing Variables Literature Recommendations", " Linear Regression – Application The Basic Command The command to run a regression in R is beguilingly simple: regression &lt;- lm(devar ~ indepvar, data=dataframe) We specify an object into which we store the results of the regression, here regression, and assign a function to this object called lm which stands for “linear model”. It is then convention to state the dependent variable first in the command, which in our case will always be democracy. This is then followed by a tilde and the independent variable which you want to include. In the case of modernisation theory, you might want to test what influence per capita GDP has on democracy. This week we are using the Polity V scale to measure democracy. “The ‘Polity Score’ captures [the] regime authority spectrum on a 21-pont scale ranging from -10 (hereditary monarchy) to +10 (consolidated democracy). (…) The Polity scheme consists of six component measures that record key qualities of of executive recruitment, constraints on executive authority and political competition. It also records changes in the institutionalized qualities of governing authority.”4 We start by setting the working directory setwd(&quot;~/Warwick/Modules/PO3B3//Week 2&quot;) and then load the Europe.csv data set into the workspace. We subset this data frame to observations from the year 2000, only. europe &lt;- read.csv(&quot;Week 2/Europe.csv&quot;) library(tidyverse) europe_2000 &lt;- filter(europe, year == &quot;2000&quot;) Our dependent variable is called polity. The independent variable is called gpdpc. We are now ready to run our first regression: reg_pol &lt;- lm(polity ~ gdppc, data=europe_2000) We can then produce a summary of the results as follows: summary(reg_pol) Call: lm(formula = polity ~ gdppc, data = europe_2000) Residuals: Min 1Q Median 3Q Max -14.1330 -0.6741 0.4742 1.4750 2.6515 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.997e+00 7.859e-01 8.902 8.61e-10 *** gdppc 1.068e-04 4.055e-05 2.635 0.0134 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.973 on 29 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.1932, Adjusted R-squared: 0.1654 F-statistic: 6.944 on 1 and 29 DF, p-value: 0.01336 We can extract the number of observations used for the estimation by calling: nobs(reg_pol) [1] 31 There is a lot of information in this, and I will take you through the output step by step now. Interpreting the Output The Number of Observations nobs(reg_pol) [1] 31 Letl’s deal with the last step first. The number of observations is equal to the number of countries in this case. We have subset the data to the year 2000, and so we have 33 countries in the analysis. This seems trivial for now, but it will become important later on. Once observations are missing, R drops them from the analysis – especially in developing countries where data are often missing in large quantities this can lead to a rapid decimation in the number of observations. This in turn is problematic for the strength inference we can draw from the analysis. The Intercept R always shows the value for the intercept in the intuitively labelled row “(Intercept)” and the column “Estimate”. In this case the value is 7.0. What does this coefficient mean, substantively? When you remember the graph depicting the regression line, this is the point where the line intercepts the y-axis. So, it is the value of \\(y\\), here democracy in the form of the Polity V score, when \\(x\\), here economic development in the form of GDP, is zero. So in other words, a country with a GDP per capita of zero would achieve a Polity V score of 7. The substantive interpretation sometimes makes sense (like here), but sometimes cannot be interpreted in this way. The Slope Coefficient The slope coefficient is shown in the row depicting the name of the independent variable, here “gdp”, and again the column “Estimate”. Our slope here, is \\(1.0681e-4\\). The \\(e-4\\) means that we have to move the decimal point 4 units to the left, so written fully, this means \\(0.0001068\\). We interpret it as follows: for every additional unit of per capita GDP, measured in US$, the Polity V score increases by \\(0.0001068\\), on average. This seems very small, but when you consider the size of GDP per capita in many countries, it seems logical that this value is as small as it is. If the coefficient was negative, then this would mean that for for every additional unit of per capita GDP, measured in US$, the Polity V score would decrease by \\(0.0001068\\), on average. The all-important p-value in Regression Everybody is obsessed with the p-value in quantitative research, but what does it mean in this context and where can you find it? You will have watched the video on the p-value more generally. If you haven’t, or have forgotten what it says, watch it now before reading on. First things first: how do we interpret the p-value for regression? We do a regression in order to ascertain whether there is a relationship between the independent and the dependent variable, or not. For testing whether there is one, we start from the assumption that there is none. This is what we call the null hypothesis; in our example here it would be that per capita GDP does not influence the level of democracy in a country. Now, remind yourself of the normal distribution from the video which has the mean age in its centre. In the video we were interested whether age influences height. Assuming, that there is no relationship between age and height, a regression line would look as follows: The line is perfectly flat (the slope coefficient is zero), intercepting the y-axis at the mean. This means, that for every age we expect the same height, so on average we are always right. If we now put a zero slope coefficient, such as the one for height, just in the more general form of \\(\\beta_{2}\\) in the centre of a normal distribution, it looks like this: What we want to test now, with regression, is whether the slope coefficient R has calculated for us (let me denote the estimated value of \\(\\beta_{2}\\) as \\(\\hat{\\beta_{2}}\\)), is far enough from this mean of zero, to say that we can be sure to say that there is a relationship. The statement that there is a relationship between the independent and the dependent variable, is called the alternative hypothesis. In our case the alternative hypothesis would read: “The level of per capita GDP influences the level of democracy in a country”. So how far away from the centre of zero do we have to go to say that there is indeed relationship, or put differently, that our alternative hypothesis is true? The standard in political science is that we need to have a 5% probability of finding a value more extreme than the one we have observed. Under the curve in the following graph, that is equal to the blue area on the right. And that is the p-value. If this area is 5% or less, then we have observed a value for the slope coefficient which is so far away from our assumed mean of zero, that we have sufficient evidence to reject the null hypothesis, and to accept the alternative hypothesis. But now, you might say, a slope coefficient can also be negative – here we are only looking at the right hand-side, and therefore at the scenario in which a slope coefficient is positive. And you are right. The scatter plot could give us a negative line. So, if we want to move away far enough from the assumed mean of zero in the centre, we must do so in both directions, to the left and to the right. Now, we need a value that is so far out, that to either side of the distribution, 2.5% of the area are left under the curve (2.5% on the left plus 2.5% on the right make the overall 5% we are interested in). We call this a two-sided test, whereas the scenario above is a one-sided test. The p-values reported by R for the slope coefficients are always two-sided tests (unless we tell R not to, but we are not doing that on this module). This value gives us the area under the normal distribution to the left and the right beyond our observed value, as shown in this figure: The value we are looking at in R to determine the p-value, is in the column \\(Pr(&gt;|t|)\\). In order to satisfy the requirement of the p-value being 5% or less, this value needs to be smaller than 0.05. Otherwise, more than 5% area are left, and we are not certain enough that our value is far enough away from the zero mean in the centre to say that it is “statistically different” from it. When we look at the value for the slope coefficient gdppc here, \\(0.0134\\), this means that the areas on the left and the right are jointly 1.34% – small enough for us to be sure to have found a value that is far enough away from zero to claim that there is a relationship. We therefore reject the null hypothesis, and accept the alternative hypothesis: we find evidence for a relationship between per capita GDP and Polity V in Europe in the year 2000. Again, if we want to visualise this, the actual p-value of \\(0.0134\\) would look like this: The Goodness of Fit (R-Squared) Recall from the video that the goodness of fit is a measure to indicate how much of the variation in the dependent variable (democracy) the independent variable (per capita GDP) is explaining. For this, we take the ratio of the explained sum of squares over the total sum of squares. The resulting percentage is R-Squared. This number can also be found on the R output, and is in our case \\(0.1932\\), or 19.32%. This value is not too bad for a single variable! The maximum we can explain is of course 100% with an R-Squared value of 1.0, even though this is a dream never achieved empirically. But we are stil quite some distance of this dream, and can probably do better. There surely must be factors other than per capita GDP that explain democracy in Europe in the year 2000. Choosing Variables Can I choose more than one independent variable? Yes, you can! And this is where the fun starts, because now we are getting a step closer to the real world. New modernisation posits that democracy is multi-causal, and does not rest on the influence of GDP alone. Instead, it puts forward a number of concepts that act as independent variables, one of which is health. We can measure health through Hospital beds (per 1,000 people), and include this in our model, on top of GDP. To do this we type: reg_pol1 &lt;- lm(polity ~ gdppc + hospital, data=europe_2000) You see that adding independent variables is easy, we just add them on with a plus sign. It does not matter whether the expected direction of influence is positive or negative, always add additonal variables with a “+”. The command ought to lead to the following output: summary(reg_pol1) Call: lm(formula = polity ~ gdppc + hospital, data = europe_2000) Residuals: Min 1Q Median 3Q Max -11.0906 -0.4154 0.2112 1.4259 3.5517 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.134e+01 1.903e+00 5.959 2.05e-06 *** gdppc 7.668e-05 3.934e-05 1.949 0.0614 . hospital -5.828e-01 2.361e-01 -2.469 0.0199 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.742 on 28 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.3374, Adjusted R-squared: 0.2901 F-statistic: 7.13 on 2 and 28 DF, p-value: 0.003142 nobs(reg_pol1) [1] 31 Ceteris Paribus Let us focus on the coefficient for hospital beds first. Its value is rounded \\(-0.5828\\), implying that for every additional hospital bed per 1,000 people, the Polity V score decreases by \\(0.5828\\) units on average. So far, so good, but as we have included other variables in the regression model, namely per capita GDP, we need to account for this fact in our interpretation. We do this by adding “all other things being equal” (Latin: ceteris paribus) to this interpretation. What does this mean? It means, that if we take into account the level of per capita GDP, and hold this level constant, then for every additional hospita bed, the Polity V score decreases by \\(0.5828\\) units on average. As such, we force the regression model to isolates the effect of hospital beds by including other possible explanatory factors, such as per capita GDP. You will sometimes read this in articles in the form of “controlling for”. To give you a different example: suppose we want to find out whether sex influences income. We could simply run a regression with income as the dependent variable, and sex as the independent variable. But we also know, that age influences income, as with increasing age people have more experience which is reflected in their salary. So even though we are not interested in the amount age influences income, we would include it in the regression model, so as to isolate the effect of the variable we are interested in: sex. Back to our modernisation example and life expectancy. The p-value for this coefficient is \\(0.0199\\), and therefore well below the required 5% threshold. We can conclude that the number of hospital beds influence the level of democracy in Europe in the year 2000. Per capita GDP is rendered insignificant, however. Parsimony But can you just add independent variables at your leisure? The short answer is no. The long answer is: parsimony. This means “as few as possible, as many as necessary”. The “necessary” component is guided by the theoretical underpinning of your investigation. For example, you subscribe to new modernisation theory, and believe that that it is not only economic development in the form of per capita GDP that determines the level of democracy, but that indicators of social change also play an important role. Now it is your job as a researcher to decide how we measure social change. Do we include education? And if so, how do we measure it, say by literacy levels? Should we choose a different measure for health that measures it more directly than hospital beds? Then we might decide on life expectancy. But are these two enough to measure social change, or do we need to look at other facets? We seek to include as few as possible to produce an empirical picture of social change, but so many that we are doing proper justice to the theory. We can then proceed to test different scenarios. For example, does economic development already explain democracy? What happens if we add social change? Or does social change explain democracy on its own, already? These questions lead to the topic of “model specification”, which we will discuss in greater detail in week 4. R-Squared Again As soon as we introduce more than one independent variable to the model, we cannot use “Multiple R-Squared” any more. The reason is that this measure cannot properly take into account added variables. It will either stay the same, or increase, it cannot decrease. This of course, makes no sense, for example if we add average shoe size in 2000 to the analysis, this would not help to explain democracy, but Multiple R-Squared would still likely go up. We therefore need to new measure, calles “Adjusted R-Squared” which not only penalises us for adding more indepenent variables, but will also decrease if a variable takes explanatory power away from a model. You find it here: Literature Recommendations Fogarty (2023): Chapter 11, read Chapter 12 if you want to be really good Stinerock (2022): Chapter 12 and Sections 13.1-13.5, 13.8 and 13.10. For more detail see .↩︎ "],["important-disclaimer.html", "Important Disclaimer", " Important Disclaimer I have discussed linear regression with you this week for two reasons: I am trying to sketch the methodological development through which the relationship between economic development and democracy has been analysed over time. Lipset (1959) started with a simple correlation analysis, and this quickly developed into linear regression analysis, as it is more powerful and more sophisticated than correlation analysis. It forms the foundation for the binary response models (probit in our case) which we will start with next week. To start with, we will only look at the probability to be democratic in a particular year across different countries. But in Week 7, we will extend this to not only investigate this relationship across countries, but also over time. This dynamic probit, or Markov Transition Model (MTM), is the model you need to apply in the assessment of the module. Please DO NOT stick to linear regression in the assessment, as OLS is not capable to deal with time-series data. The same applies for a cross-sectional probit. The only method permissible int he assessment is an MTM. "],["probit---theory.html", "Probit - Theory Probit - What is it?", " Probit - Theory Last week, we encountered linear regression analysis which allowed us to quantify the amount and direction of one or more independent variables on a continuous dependent variable. I already mentioned there, that there is also a type of a regression which can deal with a binary dependent variable. This is usually a yes/no scenario, such as democracy / autocracy, war / peace, trade agreement / no trade agreement, … You get the picture. Many problems or questions in political science have binary outcomes, and so you are about to learn a very important and useful method to answer research questions. As in the previous Chapter, I will take you some through some theory first, and then we are applying the theory to an empirical example. This time concerning the survival of passengers on the Titanic. Probit - What is it? A question we can all relate to is whether to go out tonight, or not. The “propensity to go out” is not directly observable, and so we call this a latent variable. You can imagine this running from minus infinity to plus infinity, and at some point on this continuum you are making the decision to go out. Let’s call this point tau (\\(\\tau\\)). Graphically, this would look like this: Figure 8: Latent Variable Your inclination to go out, is likely to be influenced by the amount of money you have in your wallet / bank. If you are broke, you will be less inclined (if you are sensible), and if you are swimming in it, you will be more inclined. So, if the “propensity to go out” (which remember is running from minus to plus infinity) is influenced by your budget, then let’s construct a graph, in which we pop the propensity to go out on the y-axis, and the budget on the x-axis. If we assume that this relationship is linear, we can fit a regression line into this coordinate system, just as we have in the previous week: Figure 9: Effect of Budget on Propensity to Go Out Whilst this visualises the influence of the budget on the latent variable, what we are aiming for is to make a prediction about the probability of you going out, or not. Now imagine, your budget is \\(x_{1}\\). The regression line depicts the propensity that we would expect to see, on average, for somebody with a budget of \\(x_{1}\\). But the crucial point is that not everybody is average. Some might have an essay deadline approaching which makes them even less likely to go out. Others might just have recieved their essay mark, and want to celebrate that they scored a first. In other words, there is variability around the regression line. And we assume that whilst this variability is random, it still follows a particular distribution. In the case of a probit, this is the normal distribution5. I have added these distributions to Figure 10. As you can see, even at budget \\(x_{1}\\), some area of the distribution has slipped over the cut-off point, \\(\\tau\\). Figure 10: Towards the Cumulative Density Function, adapted from J. S. Long (1997, p. 46) The probability of going out is coloured in in grey. You can see that even at \\(x_{1}\\) there is a teeny bit of probability that you will go out. As the budget increases, more and more probability slides over the threshold \\(\\tau\\), until we reach the magical point of \\(x_{5}\\) where the probability is 50%. From there on, the amount of probability sliding over \\(\\tau\\) is steadily decreasing, because of the shape of the normal distribution. We can depict the amount of probability (or the size of the grey area) for each \\(x_{i}\\) in a separate graph which is called the Cumulative Probability Density Function, or short CDF: Figure 11: The Cumulative Distribution Function, adapted from J. S. Long (1997, p. 46) This s-shaped curve now gives us the probability (of going out) for each \\(x_{i}\\) (budget). It is important to note that the relationship is not linear, as in linear regression. Because we have an s-shaped curve the increase in probability when going from \\(x_{2}\\) to \\(x_{3}\\) is not the same as going from \\(x_{3}\\) to \\(x_{4}\\). You can see that visualised here: Figure 12: Marginal Effect under the Cumulative Density Function We will therefore not be able to interpret the coefficients in the same way as for OLS. We will be using predicted probabilities instead. But one step at a time. Let’s first get our hands dirty with some data. For logit, the logistic distribution is used. This would lead to very similar results.↩︎ "],["probit-application.html", "Probit – Application The Command The Coefficients Pedicted Probabilities How to Report Results Literature Recommendations", " Probit – Application The Command The command to run a probit is as follows: model &lt;- glm(depvar ~ indepvar data = dataframe, family = binomial(link = &quot;probit&quot;)) glm stands for “General Linear Model”. We then specify a model in the same way as last week, stating the dependent variable first, followed by a tilde and then the independent variable(s). We complete the command by naming the data frame we wish to use and selecting the model type, in our case it is a probit model which is binomial (our dependent variable only has two possible outcomes). Once again, let us do a specific example with the data set “Europe” in the year 2000. First set the working directory for today’s seminar: setwd(&quot;~/Warwick/Modules/PO3B3/Week 3&quot;) and then load the data set into a data frame called world which we subset to the year 2000. world &lt;- read.csv(&quot;files/Week 3/world.csv&quot;) library(tidyverse) world2000 &lt;- filter(world, year==2000) For a probit regression, we cannot use the Polity V scale any more, because it is continuous, and not binary. We therefore switch the democracy coding to index created by Boix, Miller &amp; Rosato in 2018. We will use life expectancy as our independent variable, and examine the situation in the year 2000. To do this, we call: probit &lt;- glm(democracy ~ life, data = world2000, family = binomial(link = &quot;probit&quot;)) This should lead to the following results: summary(probit) Call: glm(formula = democracy ~ life, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.73066 0.72586 -5.140 2.75e-07 *** life 0.05793 0.01078 5.375 7.66e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 252.11 on 182 degrees of freedom Residual deviance: 220.10 on 181 degrees of freedom (8 observations deleted due to missingness) AIC: 224.1 Number of Fisher Scoring iterations: 4 The Coefficients The coefficients are displayed in the results as follows: But how do we interpret them? As you have seen in the Theory section above, the relationship between our independent variable and the probability of democracy is not linear: the curve was s-shaped, so that the increment in probability is not the same for, say moving from \\(x_{1}\\) to \\(x_{2}\\) and from \\(x_{2}\\) to \\(x_{3}\\). For illustration see the red bars in the following Figure: What we therefore need to do in the world of probit, is to evaluate the probability at individual values of \\(x_{i}\\), or put differently, assess how much of the bell-shaped curve has slid across our cut-off point \\(\\tau\\) (tau) at a particular point \\(x_{i}\\). And this brings us to predicted probabilities. Pedicted Probabilities Once we have estimated the model, we have determined the shape of the s-shaped curve at the start of the chapter. What we now need to do, is to evaluate the probability on the y-axis for different values on the x-axis. In our case this is the variable life. Setting the x-values Let us first get a basic overview of the variable life. We can do this by calling summary(world2000$life) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 44.52 59.46 69.58 66.49 73.84 81.08 8 So we know that the average life expectancy in the world in the year 2000 was 66.49 years,with a minimum of 44.52 years, and a maximum of 81 years. We also have eight missing observations (NA) which we need to exclude from the following by setting na.rm=TRUE6, as R is otherwise unable to calculate descriptive measures within the setx function, such as the mean. Let us set life to its mean now, by typing: setx = data.frame(life=66.49) If you want to change this value for calculations later on, you simply set different values in the setx dataframe or specify a new one). We now have the shape of the probability curve, and we have agreed on a point on the x-axis. We are finally ready to have a look at the probability to be a democracy at this point. Predicting the Probability The quantity of interest we are interested in is the probability of being a democracy. To calculate this quantity of interest, we use the function predict() and specify within this function for which model we want to calculate the quantity, and the data frame in which we specified the value of our x-variable. In our case this variable is life and we have assigned the mean value to an object called setx earlier. predict(probit, setx, type=&quot;response&quot;) 1 0.5481741 Remember that we have put the value of life expectancy at the mean. For this level of life expectancy R returns to us a probability of being a democracy at 54.82%. Contrarily, this means that probability to be an autocracy is 45.18% (the two probabilities always sum up to 1). Now we set life expectancy to its minimum setx = data.frame(life=min(world2000$life, na.rm = T)) and calculate the quantity of interest again. Our probabilities have changed very drastically (ensure you get the same results before proceeding): predict(probit, setx, type=&quot;response&quot;) 1 0.124708 If we set life expectancy to its maximum (how?), then we receive the following regime probabilities: predict(probit, setx, type=&quot;response&quot;) 1 0.8329802 Now we can make statements such as: In 2000, a country’s probability to be a democracy with average life expectancy was 54.82%. At the minimum life expectancy of 44.52 years, this probability drops by 42.35%age points to 12.47%. As such, a country in 2000 at minimum life expectancy would more likely be an autocracy than a democracy (why?). All Predicted Probabilities If you want to create a graph which depicts the predicted probability for all value sof your independent variable, then you can download the RScript for creating such a graph here. It will look like this: How to Report Results The question is now: how do you report all of this in an article, or closer to home, in your assessment? Let us start by calculating a model which assesses the impact of per capita GDP on the probability to be a democracy in 2000, worldwide. The R commands and output look like this: probit &lt;- glm(democracy ~ gdppc, data = world2000, family = binomial(link = &quot;probit&quot;)) summary(probit) Call: glm(formula = democracy ~ gdppc, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.621e-01 1.161e-01 -1.396 0.163 gdppc 5.128e-05 1.309e-05 3.918 8.93e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 248.48 on 180 degrees of freedom Residual deviance: 225.62 on 179 degrees of freedom (10 observations deleted due to missingness) AIC: 229.62 Number of Fisher Scoring iterations: 5 Now, please, please never, ever copy this into an article or an assessment, as every time you do this, a little part of me dies. Make the effort of reporting the results in a nice and neat Table, that only contains all relevant information, communicates it in an accessible form, and is clearly labelled. The output above, processed properly, would look like this:     Table 2: Influence of per capita GDP on Democracy Dependent variable: Democracy per capita GDP 0.058*** (0.011) Constant -3.731*** (0.726) Observations 183 Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01     You see, this table manages to convey clearly the relationship we are assessing, the label of the independent variable (not in the form of cryptic variable names), the value of the intercept, the value of the slope coefficient, their respective p-value, and the number of observations. It also has an informative caption underneath and is properly labelled (so that it can be cross-referenced in the text). As such, the table itself could already communicate the main take-away message without somebody looking at the text. This needs to be the goal: the table, or figure needs to be able to communicate the message without having to look at the text. In turn, the text needs to be able to communicate the message on its own without looking at the table. But both need to say the same thing. After you have reported the results of the regression output in this way, you can then proceed to interpret (in the text) the transition probabilities as discussed above. Again, you might wish to sum up the main results in tabular format. Stargazer Rather than setting tables manually in Excel, or even worse in Word (it basically violates all standards of professional table formatting), you can let R do this work for you. All you need is a magical package called stargazer. I have provided some Sample Stargazer Code in the Downloads Section. Open it now, and try to understand it, I have annotated it. But here are some pointers: The stargazer function needs to contain, in order: the name of the R object where you stored the regression results, the option header=F to suppress the annoying immortalisation of the author, the option type=\"html\", and the option out=\"documentname.doc\" which places a word document with that file name in your working directory. Should you use MS Word to write your essay and your essay is saved in your working directory, do not save the table document under the same name as your essay, as R will overwrite it and it will be gone forever. Beyond these basic elements I have added some lines in the sample code to improve the table. For example, I replaced the variable names with the variable labels, and suppressed unwanted statistics. You could also add a name for the model, or give the table a title. There is some good documenttation available for this on this cheat sheet. It is worth investing some time into this. There is a learning curve at the beginning, but it will make your life so much easier further down the line. Multiple Independent Variables As in multiple linear regression, you can also have multiple independent variables in a probit model. Schematically this would look as follows: probit &lt;- glm(depvar ~ indepvar1 + indepvar2 + indepvar3, data = dataframe, family = binomial(link = &quot;probit&quot;), The fun starts when you begin to calculate probabilities for different values for all of these independent variables. Setting the x-values You can set the value for each variable individually, such as in this little example: setx = data.frame(gdppc=min(world2000$gdppc, na.rm = T), life=66.49) Predicting the Probability This is very much the same as above with one independent variable. It is crucial to remember which variables you have set at which value, so that you can interpret the probability values correctly. As in linear regression, the interpretation of individual variables is ceteris paribus. It therefore makes sense, if you wish to isolate the effect of one, single variable, only to vary the values of that variable, and to leave all the other ones the same. If you varied two variables at the same time, for example, you would no doubt see a change in probability, but you could not attribute it to a single independent variable, any more. Literature Recommendations Fogarty (2023): Chapter 13, pp. 268-295 Stinerock (2022): Section 13.12 J. S. Long (1997); More Technical and in-depth if you want some background This stands for “not available remove equals true”.↩︎ "],["exercises.html", "Exercises", " Exercises Use the global data set. Estimate a model assessing the impact of per capita GDP and “Adjusted net enrolment rate, primary (% of primary school age children)” in 2000. Design a Table to report results in MS Word. What is the probability to be a democracy evaluated at the mean for both independent variables? What is the difference in probability ceteris paribus for minimum and maximum per capita GDP? What happens if we replace net enrolment rate by life expectancy in the model? "],["missing-data.html", "Missing Data Options in R Methodological Implications", " Missing Data Options in R Missing data is a real problem in doing empirical research. What compounds this problem is that developing countries are more affected than developed ones. Ironically, we are more interested in developing countries in this module, and so this is going to become a real-life struggle for you over the coming weeks. R has multiple ways in which to address missing data. We have already encountered the exclusion of missing values when using descriptive statistics within the setx function in Week 3, such as: mean(europe2000$lifeexp, na.rm=TRUE) When we run regression models, for example through the function lm, R let’s you specify what you wish to do with observations (rows) that have missing values in them with the function na.action. This option has four logical options7: na.fail: Stop if any missing values are encountered na.omit: Drop out any rows with missing values anywhere in them and forgets them forever. na.exclude: Drop out rows with missing values, but keeps track of where they were (so that when you make predictions, for example, you end up with a vector whose length is that of the original response.) na.pass: Take no action By default, R would apply na.omit which we call “listwise deletion”. This means, that as soon as one value within an observation is missing, R drops that entire observation. This can potentially decimate the number of observations for analysis quite drastically! This in turn has implications for inference. Let me illustrate this. Methodological Implications In an ideal world, we would have a data set in which each value is present (indicated by the presence of an x in each of the cells of this table): Unfortunately, in the real world, we are much more likely to have a Swiss cheese, such as this: Figure 13: Schematic of a real data set You will already have seen this when opening the data sets we are working with. In Figure 14, for example, the variable gdppc only has three missing values, but agri is completely missing. Figure 14: Excerpt from the world data set If you ran a regression model that uses the variable agri listwise deletion would delete all observations and you would end up with no model. Here, the amount of observations you would lose is obvious. But since different variables have missing values in different places, it is often not as obvious as that. Assume, for example that we wished to use all four variables in our Schematic in Figure 13. Even though the “missingness” problem looks much less pronounced than in the agri variable in the world data set (see Figure 14, listwise deletion would ensure that we would only be left with two observations out of ten in our model: So what do you do when you find yourself in such a situation? An obvious answer would be to simply leave out a variable that has a high degree of missingness. This solution is flawed, however. If your theory tells you that the variable plays a role in measuring one of the concepts then leaving it out would create so-called omitted variable bias – a problem you really don’t want to have. A much better solution, therefore, is to use a proxy variable. Take agri as an example. It measures the percentage of GDP generated by agriculture, and would probably enter a statistical model to measure the transition of a traditional society (agricultural) to a modern, industrialised society. So, rather then measuring the move away from agri, you would also measure the transformation into industry. If you can find a variable measuring the percentage of GDP generated by the industry sector, and this variable has fewer observations missing, you could you it instead of agri and still be measuring the same thing. Sometimes, it is not as straightforward as this, however, and you will have to resort to variables that have less measurement validity. Assume, for example, you want to measure the populations ability to read. Then literacy from our data set would be an obvious choice. But sadly, it has a high number of observations missing. An alternative is primary gross enrolment (enrol_gross), as pupils learn to read and write in primary school. This variable does not measure the characteristic we are interested in as directly as literacy, but it would allow us to include more observations in our model. It is an art to balance measurement validity against the number of observations, there is no hard and fast rule to help you make this decision. But as long as you discuss and explain your reasoning in the assessment, you will be fine for this module – after all, you are only starting out on this. https://faculty.nps.edu/sebuttre/home/R/missings.html↩︎ "],["model-building.html", "Model Building Control Variables Literature Recommendations", " Model Building When we are testing a theory, such as modernisation theory, then we usually have a plethora of independent variables to measure the concepts involved in that theory. Take economic development. In its origins, modernisation theory saw development almost purely as economic growth, a notion enshrined in what we refer to now as Classical Modernisation Theory. But as our understanding of what constitutes “development” has changed over time, so have the propositions of modernisation theory. Diamond (1992) made an important contribution to the literature by proposing that wealth, such as per capita GDP, really is only a means to facilitate social change, such as increasing education levels, allowing people to look after their health, etc. He posited that it is these social changes that facilitate democracy. Wealth is necessary, but it is only a first step. Modernisation theory, therefore, can be seen as having to parts, the classical, and the new one. When you look at journal articlaes, you will notice that these parts are usually tested separately in regression models, first just the classical component, then just the social component, then both of them together. This is an important strategy in regressiona nalysis, as it allows us to isoloate the explanatory power of not only each part of the theory, but also of individual variables. Let me illustrate what I mean by this. First, we need to decide how to actually measure “economic development” and “social change”. There are a lot of options: Economic Development GDP Agricultural Land Access to electricity Mobile Phone Subscriptions … Social Development Education: literacy, primary completion rate, school enrolment, etc. Health: life expectancy, tuberculosis incidents, health expenditure, etc. … Once we have selected our variables, we start by runnign bivariate models between each independent variable and democracy as the dependent variable. This give sus information if each variable is individually able to explain democracy, and if so, how well. Then we start to combine independent variables into multivariate models, to test different combinations. This will allow us to see if one independent variable might take explanatory power away from another, and as such explains democracy better. I have done this in the following table: Table 3: Impact of Socio-Economic Development on Democracy Dependent variable: Polity V (1) (2) (3) per capita GDP 0.0001*** 0.0001* (0.00003) (0.00003) Life Expectancy 0.160*** 0.090 (0.054) (0.066) Constant 2.584*** -7.373** -3.248 (0.605) (3.691) (4.339) Observations 150 152 150 R2 0.064 0.056 0.075 Adjusted R2 0.057 0.050 0.063 Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 As you can see, both per capita GDP and life expectancy can explain variation in the Polity V score in 2007 (Models 1 and 2, repsectively). But when we combine the two in Model 3, life expectancy loses its significance, whilst per capita retains (an admittedly lower level of) significance. In these three models, we have thus discovered evidence that Diamond was wrong: When we look at the role of per capita GDP and life expectancy simultaneously, per capita GDP is able to explain democracy whilst life expectancy is not. How would you make use of this in the assessment? It goes without saying that you will have to run a lot of models to find such a story. But it would make little sense to overburden the reader of your assessment with all of these models (quite apart from constraints on your word count). Instead, you would only include in your assessment those regression models in the results tables which allow you to tell this story. Note that in this case, I have also quite neatly tested both the classical and new approach to mdoernisation theory. Model 1 is classical modernisation, Models 2 and 3 test new modernisation theory. We have found evidence that classical modernisation theory is more applicable in this scenario than new modernisation. Control Variables The selection of independent variables MUST be guided by theory. After all, this is our purpose in running regression models: finding out whether a particular theory can explain an empirical phenomenon we are witnessing. If you select variables that are irrelevant to your theory, then the research design breaks down, as you are no longer focusing on testing your theory. I know it sounds trivial, but you would be surprised how many students mess this up in assessments. But there is one – and only one – exception to the rule of not including variables that are not motivated by the theory you are testing: so-called control variables. For example, we suspect that the amount of official development aid (ODA) a country has received might influence democracy. Modernisation theory is only concerned with processed WITHIN a country, and so a flow of money coming from outside of the country is not part of the theory. Nonetheless, such funds can either facilitate development, in the sense of helping to build infrastructure such as roads, etc. Or it can be used for democracy promotion directly (the World Bank does tie some of its ODA funds to this purpose), through funding relevant institutions, facilitating elections, etc. So, even though these external funds have no place in our theory, we have strong reason to believe that they affect the dependent variable. And it is for this reason, that we would still include them in our regression model to control for their influence. Why? The principle of ceteris paribus (see Week 3) only applies with respect to those variables included in the model. And if we wish to control for ODA, or to purify those coefficients which are motivated by theory from the influence of ODA, we need to include this variable in the model in order to do so. Even though I am presenting this here as the last step, you would realistically select these variables together with the ones motivated by the theory, as you will also have to perform conceptualisation and measurement, check data availability, etc. Literature Recommendations Fogarty (2023): Chapter 11: Section “Multiple Regression and Model Building” Stinerock (2022): Section 13.11 "],["consolidation.html", "Consolidation The World-Value Survey", " Consolidation There is no new methodological material to give you a bit of a rest. Instead, we will connect what we have learned so far in terms of methods with the theory of cultural modernisation. For our investigation we will be using the World Value Survey. The World-Value Survey The World Values Survey () is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS Association and WVSA Secretariat headquartered in Vienna, Austria. The survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world?s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones. The WVS has taken place across six waves, each collecting data from a different set of countries: "],["exercises-1.html", "Exercises Descriptives Linear Regression Probit", " Exercises Descriptives For all exercises, use the “WVS” data set which is available in the Downloads Section. wvs &lt;- read_dta(&quot;wvs.csv&quot;) What is the average value for survival/self-expression values? What does this mean? Filtering through Waves, how has this average changed? Repeat these two steps with traditional/rational values. Why is the comparison of these values over time difficult? Linear Regression For which waves do traditional–rational values in the population explain a country’s level of democracy? For which waves do survival–self-expression values in the population explain a country’s level of democracy? Identify reasons why earlier waves fail to explain the level of democracy. Assessed jointly, how much more or less democratic do survival/self-expression value and GDP growth make countries in wave 5 (2005-2009)? Probit For which waves do traditional–rational values in the population explain a country’s probability to be a democracy? For which waves do survival–self-expression values in the population explain a country’s probability to be a democracy? Identify reasons why earlier waves fail to explain regime type. Calculate a model assessing the impact of GDP growth on the probability of democracy for countries in wave 5. How much less likely is a country to be a democracy moving from minimum GDP growth to maximum GDP growth in wave 5? Calculate a model assessing the impact of traditional–rational values, and GDP growth on the probability of democracy for countries in wave 5. Interpret the results. Considering the main propositions of cultural modernisation, what are the implications of these results? Assess the results of each of the previous tasks in turn. ## Solutions {-} You can find the solutions to these exercises in the Downloads Section. "],["markov-transition-models.html", "Markov Transition Models Time Series, Cross-Sectional Data What are Markov Transition Models? Democratic Emergence Democratic Survival Interpretation", " Markov Transition Models Time Series, Cross-Sectional Data We are now entering the real world, as we will start to look at how characteristics vary, not only across different countries, but also across time. I have already presented the structure of time-series, cross-sectional (TSCS) data in Week 1, but to jog your memory, here it is again: As you can see, each country has multiple observations, one for each year in which data have been observed. We will be using this information this week to calculate probabilities of regime transitions. For example, what is the probability of a country to transition from autocracy to democracy? As we only observe the regime type once for each year, this question alludes to the difference in regime type between two years in the same country. We will do this with a so-called Markov Transition Model. What are Markov Transition Models? A Markov Transition Model (MTM) models the probability of being a democracy in a particular year, given its regime type in the previous year. So, if we model the probability of a country to be a democracy this year, given that it was an autocracy in the previous year, we are modelling the probability of a transition from autocracy to democracy. Similarly, if we model the probability of a country to be a democracy this year, given that it was also a democracy in the previous year, we are modelling the probability of democratic survival. We can express these probabilities as so-called conditional probabilities. Conditional Probabilities Conditional probabilities express what I have described before in a formal way. The conditional probability to model for democratic emergence is written as follows: \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation}\\] This reads: The probability of a country i to be a democracy (y=1) in year t, given (this is what the vertical line | says) that country i was an autocracy (y=0) in the previous year (t-1). Analogously, the conditional probability for democratic survival is: \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation}\\] This reads: The probability of a country i to be a democracy (y=1) in year t, given that country i was also a democracy (y=1) in the previous year (t-1). We will now apply this knowledge to create a model in R which calculates these conditional probabilities. Let’s start with democratic emergence. Democratic Emergence Democratic emergence is expressed as the probability of a country to be a democracy in year \\(t\\), given that it was a dictatorship in the previous year, \\(t-1\\) \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation*}\\] As a first step, we therefore need a variable that gives us the information which regime type each of our countries had in the previous year. We will use the world data set for this illustration. world &lt;- read.csv(&quot;files/Week 7/world.csv&quot;) library(tidyverse) We can now create the lagged democracy value. In order for R to know when a new country “starts”, we need to group observations by country first, and then lag the variable democracy. We do this with the intuitively called function lag(). We then ungroup the data again, as we have no need of the groups any more. world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.democracy = lag(democracy)) %&gt;% ungroup() This creates a new variable l.democracy which gives us the information we were after: the regime type of the country in the previous year. Note that as the first observation for each country cannot be lagged, it creates as many missing values as we have countries in the data set. Perhaps this makes more sense with a visualisation: Lagging the Independent Variables So far, we have lagged the dependent variable, to run an MTM. This was a methodological necessity. But from a substantive point of view it makes sense to also lag our independent variables. It is reasonable to assume, that the regime type in year \\(t\\) depends on the state of socio-economic development in the previous year, \\(t-1\\). It is rare that for example a recession hits, and the country immediately changes regime type. These things need time. And this is why we will now also lag the independent variables. This is the same procedure as before. For “per capita GDP” we call: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.gdppc = lag(gdppc)) %&gt;% ungroup() Let’s do the same for life expectancy: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.life = lag(life)) %&gt;% ungroup() and Primary gross enrolment rate: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.enrol_gross = lag(enrol_gross)) %&gt;% ungroup() Subsetting the Data for Conditional Probabilities Recall that for democratic emergence we need all of those observations in which a country has been an autocracy in the previous year. We can select these in R by filtering the data: world_democ0 &lt;- filter(world, l.democracy==0) Whilst we are at it, we might as well also do this for democratic survival. Here we need all observations in which l.democracy=1 world_democ1 &lt;- filter(world, l.democracy==1) And with that we are ready! Democratic Emergence Even though “Markov Transition Model” sounds very complicated, the code in R is actually no different from a “regular” probit model. The only difference is that we have selected a certain type of observations. For emergence we call: emergence &lt;- glm(democracy ~ l.gdppc, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(emergence) Call: glm(formula = democracy ~ l.gdppc, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.967e+00 5.170e-02 -38.052 &lt;2e-16 *** l.gdppc -3.201e-05 1.558e-05 -2.054 0.04 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 794.78 on 3879 degrees of freedom Residual deviance: 787.79 on 3878 degrees of freedom (1099 observations deleted due to missingness) AIC: 791.79 Number of Fisher Scoring iterations: 8 Once again, we can include multiple independent variables by connecting these with + in the glm() function: emergence_full &lt;- glm(democracy ~ l.gdppc + l.life + l.enrol_gross, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(emergence_full) Call: glm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.706e+00 3.683e-01 -7.346 2.05e-13 *** l.gdppc -7.424e-05 2.906e-05 -2.554 0.0106 * l.life 1.810e-02 7.848e-03 2.306 0.0211 * l.enrol_gross -2.358e-03 2.510e-03 -0.939 0.3476 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 664.33 on 2845 degrees of freedom Residual deviance: 650.69 on 2842 degrees of freedom (2133 observations deleted due to missingness) AIC: 658.69 Number of Fisher Scoring iterations: 9 Democratic Survival Modelling democratic survival is easy now, as we have already completed all necessary preparations. To model \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation*}\\] we call: survival &lt;- glm(democracy ~ l.gdppc, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(survival) Call: glm(formula = democracy ~ l.gdppc, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.627e+00 8.555e-02 19.019 &lt; 2e-16 *** l.gdppc 2.268e-04 4.788e-05 4.738 2.16e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 583.28 on 3791 degrees of freedom Residual deviance: 503.78 on 3790 degrees of freedom (385 observations deleted due to missingness) AIC: 507.78 Number of Fisher Scoring iterations: 11 And again with all independent variables: survival_full &lt;- glm(democracy ~ l.gdppc + l.life + l.enrol_gross, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(survival_full) Call: glm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 8.398e-01 6.069e-01 1.384 0.16645 l.gdppc 1.373e-04 4.842e-05 2.836 0.00457 ** l.life 7.953e-03 1.182e-02 0.673 0.50122 l.enrol_gross 4.130e-03 3.675e-03 1.124 0.26105 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 414.00 on 2915 degrees of freedom Residual deviance: 357.24 on 2912 degrees of freedom (1261 observations deleted due to missingness) AIC: 365.24 Number of Fisher Scoring iterations: 11 Interpretation The interpretation is analogous to a cross-sectional probit model we explored in Week 3. Just bear in mind that the predicted probabilities are all indicating the probability for democracy to emerge, or for democracy to survive. "],["model-fit-in-binary-response-models.html", "Model Fit in Binary Response Models8 How to Measure Model Fit", " Model Fit in Binary Response Models8 How to Measure Model Fit There are quite a few model fit measures around for binary response models, such as pseudo R-Squared and the Akaike Information Criterion (AIC). What I am going to present here, however, is the so-called ROC curve, as I personally really don’t like the aforementioned measures. What is it? The principle idea of ROC is to determine how well our model is able to separate cases into the two categories of our dependent variable. It approaches this question by comparing the actual observed values of the dependent variable with the values the model would predict, given the values of the independent variables. Consider Figure 15 a). On the x-axis you see eight observations, marked as coloured diamonds. Red diamonds represent countries which are dictatorships, and blue ones democracies. They are sorted by their respective level of per capita GDP. Now suppose that the displayed CDF is the results of a model we have estimated. With a cut off point \\(\\tau=0.5\\) we would correctly predict the group of four observations on the left to be dictatorships. They are True Negatives (TN). The group on the right would be correctly predicted as democracies, these are True Positives (TP). We have no incorrectly classified cases; our model has been able to separate cases perfectly. You can see this represented in the form of distributions in panel b). Figure 15: Perfect Separation As you well know by now, the real world is oddly deficient in achieving perfection such as this. We will observe both poor democracies, and rich dictatorships. This scenario is shown in Figure 16 a). Figure 16: Overlap According to the CDF we would predict the poor democracy as a dictatorship. It would be a False Negative (FN). Conversely, we would predict the rich dictatorship as a democracy and would obtain a False Positive (FP). The distribution of cases in Figure 16 b) is not as clearly separated any more as in Figure 15 b). Now they overlap, leading to incorrect classifications. These are marked accordingly in Figure 17. As we are no longer operating in a world in which we only have TNs and TPs, I think we can all agree that our model fit is no longer as good as in Figure 15. Figure 17: False Negatives and False Positives But there is another issue: whilst setting \\(\\tau\\) at 0.5 makes intuitive sense, there is nothing preventing us from shifting \\(\\tau\\) around. Indeed, the number of FPs and FNs very much depends on where we place our cut-off point. For example, if we don’t want any FNs, then we just have to shift \\(\\tau\\) sufficiently downwards. Or if we want to avoid FPs we only need to move it far enough upwards. I have illustrated this in Figure 18 a) and b), respectively. Figure 18: Shifting \\(\\tau\\) These are only three options of placing the threshold. But we have a total of eight observations which means that there are nine potential positions for the cut-off point with each leading to a different conclusion of how well our model fits the observed data. Let’s go through this more systematically and start by placing \\(\\tau\\) at the very bottom. Because the number of TNs, TPs, FNs, and FPs will only change at the “next” observation we only need to shift as many times as there are observations. For each shift we record the values of all four quantities in a crosstabulation which is displayed in the following table: These tables are useful in their own right, but since we will end up with as many of them as there are observations, this can quickly get very messy. There are only eight observations here, but imagine doing these for the world data set with 190 observationsWe therefore need to find a way to condense the information contained in each of the n confusion matrices into a single measure. If you look closely, you will see that TNs and FPs form a close relationship: if we shift up \\(\\tau\\) in Figure 18 a) we are reducing the number of FPs and obtain more TNs. As TPs relate in the same way to FNs, we can quantify their respective relationships in the following rates: \\[\\begin{equation} \\text{False Positive Rate}=\\text{Sensitivity}=\\frac{\\text{False Positives}}{\\text{False Positives}+\\text{True Negatives}} \\end{equation}\\] In our case you can think of the False Positive Rate (FPR) as the proportion of incorrectly specified dictatorships. \\[\\begin{align} \\text{True Positive Rate}&amp;=1-\\text{Specificity}=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}\\\\[10pt] \\text{Specificity} &amp;=1-\\text{True Positive Rate}=1-\\frac{\\text{TP}}{\\text{TP}+\\text{FN}} \\nonumber \\end{align}\\] For our example the True Positive Rate (TPR) represents the proportion of correctly specified democracies. If we calculate these rates for each of our n confusion matrices, we are already reducing four quantities into two. Let’s do it. To provide a visual aid in this rather laborious process, I have created Figure 19 which depicts all eight shifts. Figure 19: Towards the ROC Curve The following table displays the confusion matrix for each of the panels in Figure 19, as well as the respective TPR and FPR (you are welcome). As we only need the TPRs and FPRs going forward, it makes sense to collect these in their own little table: We are nearly there! The last step is to display all these values in the form of a curve with the TPR on the y-axis, and the FPR on the x-axis. You can see the result – the ROC curve – in Figure 20 a). Figure 20: ROC Curve Note that I have added a diagonal where TPR = FPR. This is sometimes described as a model without independent variables. I like to think of the line as the graphical point where our model would not be able to separate between the two categories, at all. The further the ROC curve is away from the diagonal, the better our model is at separating the two categories. But there are two sides to the diagonal. We want it to be above the diagonal, as here the model is predicting 0s as 0s and 1s as 1s. Underneath, the prediction is inverse and 0s are predicted as 1s, and 1s are predicted as 0s. To summarize the position of the curve into a numerical expression, the Area Under Curve (AUC) is used, as shown in Figure 20 b). If the area is 100% we are correctly predicting everything. At 50% the model is incapable of separation, and at 0% the model gets everything wrong. This is very useful to compare different models. R How do you do all of this in R? Let’s start with the simple emergence model. In order to calculate a ROC curve, we need a new package, called pROC. Install it and load it. library(pROC) To calculate the ROC curve, we need a few steps: prob_em &lt;- predict(emergence, type=&quot;response&quot;) world_democ0$prob_em &lt;- unlist(prob_em) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em) auc(roc) Area under the curve: 0.5048 What does each line do? predict the probability of democratic emergence for each observation (country year) and store them in a new vector called prob_em. add the vector prob_em to the data frame world_democ0. In order to do this, we need to unlist the values in the prob_em vector. we then call the roc function inw hich we compare the predicted probabilities (world_democ0$prob_em) with the actual, observed regime type (world_democ0$democracy) and store the result in an object called roc as a last step we calculate the area under the curve with the auc() function But we can also plot the ROC curve: plot(roc, print.auc=TRUE) Note that print.auc=TRUE prints the numerical value into this plot. You can suppress it by setting it to FALSE. Let’s follow this procedure for the other models. Emergence: Full Model prob_em_full &lt;- predict(emergence_full, type=&quot;response&quot;) world_democ0$prob_em_full &lt;- unlist(prob_em_full) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em_full) auc(roc) Area under the curve: 0.5932 plot(roc, print.auc=TRUE) Survival prob_sur &lt;- predict(survival, type=&quot;response&quot;) world_democ1$prob_sur &lt;- unlist(prob_sur) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur) auc(roc) Area under the curve: 0.8317 plot(roc, print.auc=TRUE) Survival: Full Model prob_sur_full &lt;- predict(survival_full, type=&quot;response&quot;) world_democ1$prob_sur_full &lt;- unlist(prob_sur_full) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur_full) auc(roc) Area under the curve: 0.8165 plot(roc, print.auc=TRUE) This is a verbatim reproduction from Reiche (forthcoming). The text is based on an explanatory video by StatQuest with Josh Starmer. All figures are copyrighted.↩︎ "],["joint-estimation-of-emergence-and-survival.html", "Joint Estimation of Emergence and Survival Rationale Example Application in R Literature Recommendations", " Joint Estimation of Emergence and Survival Rationale It is perfectly adequate to estimate the processes of democratic emergence and survival separately. There is a more elegant approach, however, in which emergence and survival are estimated together. This is often employed in the literature, for example in the article by Boix &amp; Stokes (2003). The rationale here is that the model will, overall, use more observations and therefore this might influence standard errors and thus statistical significance. As you will see, the joint estimation requires a lot of manual calculation and general faff. I am mainly showing you this here, so that you can understand the output in aforementioned articles. If you want to employ this yourself, then you could: estimate the joint model as outlined here to check statistical significance of variables then estimate emergence and survival separately and work with those separately estimated models to calculate predicted probabilities and ROC curves So, what’s the setup? Again, we are incorporating the lagged value \\(y_{t-1}\\), to encapsulate all the history prior to period \\(t\\). There is just one little trick I need to introduce before we can start on the model: I will replace \\(y_{t-1}\\) by an indicator variable \\(I_{D}\\) in Equation (1) which assumes the value 1 if a country was a democracy in the previous year, and zero otherwise (notation adapted from Epstein et al., 2006, p. 553). Following on from this, we can write the model as follows9: \\[\\begin{equation} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t} + \\epsilon_{i,t}) \\tag{1} \\end{equation}\\] where \\(P(D_{it})\\) is the probability that a country \\(i\\) was a democracy in year \\(t\\), \\(\\Phi\\) is the cumulative normal distribution (the s-shaped distribution which you know from the introduction of probit in week 3), \\(I_{D}\\) the aforementioned indicator variable, \\(X_{i,t}\\) is an independent variable for country \\(i\\) in year \\(t\\), and \\(\\epsilon_{i,t}\\) is a zero mean stochastic disturbance (I only include this for completeness’ sake here. As this is irrelevant for us here and now, I will drop this from the following discussion, not to confuse you unnecessarily). This model is a multiplicative interaction model which allow us to model the probability of a country to be a democracy, conditional on its regime type in the previous year. The indicator variable \\(I_{D}\\), equal to \\(y_{t-1}\\) captures this information. How does this work? Let us start by assuming that in year \\(t-1\\) country \\(i\\) was an autocracy. In this case, the indicator variable \\(I_{D}\\) would be equal to zero, and therefore Equation (1) can be re-written as \\[\\begin{equation} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t}) \\end{equation}\\] In this case, the coefficient \\(\\beta_{1}\\) would only represent the impact of variable \\(X_{i,t}\\) on the probability of an autocracy transitioning to democracy. Expressed more formally, we are dealing with conditional probabilities here, in the case of \\(\\beta_{1}\\), we would obtain the impact of variable \\(X_{i,t}\\) on a country to be a democracy in year \\(t\\), under the condition that it was an autocracy in year \\(t-1\\). It is conditional on this, because we set the indicator variable to zero before. We can construct a similar scenario for the condition, that a country was a democracy in the previous year. In this case, the indicator variable \\(I_{D}\\) would be equal to \\(1\\), and we would obtain equation (1) again: \\[\\begin{equation*} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t}) \\end{equation*}\\] With \\(I_{D}=1\\), this can be simplified to: \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) X_{i,t}) \\end{equation}\\] This equation illustrates very well, that the impact of variable \\(X_{i,t}\\) on the probability of democracy to remain a democracy is now made up out of the sum of the two coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\), whereas the constant is the sum of coefficients \\(\\beta_{0}\\) and \\(\\beta_{2}\\). Example To make this more tangible, let’s look at an example. Assume we want to look at the effect of per capita GDP on democratic emergence (i.e. the transition of an autocracy to democracy), and democratic survival (i.e. the transition from democracy to democracy). In this case we would specify the model as follows: \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\tag{2} \\end{equation}\\] The coefficient indicating the impact of per capita GDP on democratic emergence is \\(\\beta_{1}\\). As illustrated above, the indicator variable \\(I_{D}\\) is zero in this case, and the equation would be reduced to \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t}) \\end{equation}\\] For democratic survival, the coefficient indicating the impact of per capita GDP would be the sum of coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\) \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\text{per capita GDP}_{i,t}) \\end{equation}\\] Application in R How does all of this look in R, and how do you apply this to a real-world scenario? Let’s do this using the above example of per capita GDP in the global data set. You have already created the indicator variable \\(I_{D}\\) in the form of the variable l.democracy. What we need next is a variable that captures \\(I_{D} \\text{per capita GDP}_{i,t}\\), so that we can calculate \\(\\beta_{3}\\). To create this variable, type world$gdppc_l.democracy &lt;- world$l.democracy * world$gdppc Now, we are ready to estimate the model. Remember, formally, this is written as: \\[\\begin{equation*} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\end{equation*}\\] We replace this in the R command with the equivalent variables: joint &lt;- glm(democracy ~ gdppc + l.democracy + gdppc_l.democracy, data = world, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) If you have done regression analysis before, and you worry about (multi-)collinearity in such a model, then please note that: Analysts should include all constitutive terms when specifying multiplicative interaction models except in very rare circumstances. By constitutive terms, we mean each of the elements that constitute the interaction term. Thus, X and Z are the constitutive terms in [this model: \\(y=\\beta_0 + \\beta_1 X + \\beta_2 Z + \\beta_4 XZ + \\epsilon\\)]. (Brambor et al., 2006, p. 66) You obtain the following output: summary(joint) Call: glm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.968e+00 5.139e-02 -38.290 &lt; 2e-16 *** gdppc -2.954e-05 1.452e-05 -2.034 0.042 * l.democracy 3.587e+00 1.001e-01 35.842 &lt; 2e-16 *** gdppc_l.democracy 2.513e-04 4.863e-05 5.169 2.36e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 10718.5 on 7731 degrees of freedom Residual deviance: 1299.9 on 7728 degrees of freedom (1724 observations deleted due to missingness) AIC: 1307.9 Number of Fisher Scoring iterations: 11 For democratic emergence, we can report the Intercept (Intercept) and the slope coefficient gdppc straight away as -1.968 and -0.00002954, respectively. The slope coefficient is significant, as the p-value is \\(&lt;0.05\\). This is in line with the findings from estimating emergence separately. For democratic survival, we need to take the sum of (Intercept) and l.democracy to obtain the intercept, and gdppc and gdppc_l.democracy to obtain the slope coefficient. This calculation will yield the same coefficients as the two-stepped analysis. The last step is the assessment of statistical significance. For the emergence model, we can once again interpret the output straight away. For the survival scenario, however, we need to test the hypothesis that for example \\(\\beta_{0}\\) and \\(\\beta_{3}\\) are jointly different from zero. As the survival effect is a joint-venture between these two coefficients, we also need to assess their significance jointly, and not just concentrate on \\(\\beta_{3}\\). This is the mistake Przeworski et al. (2000) have made in their seminal book, and this is what is discussed on the first few pages of the article by Epstein et al. (2006). To do this, we are using a post-estimation command which is testing the aforementioned hypothesis that \\(\\beta_{0}\\) and\\(\\beta_{3}\\) are jointly different from zero, called a Wald-Test. For this we need to install and load a new package, called survey. This is a regression term test, where you first need to state the object within which the results are stored (here: joint), and the two terms you want to test, preceded by a tilde and connected by a plus. Lastly we specify that we want the Wald Test. library(survey) regTermTest(joint, ~gdppc+gdppc_l.democracy, method=&quot;Wald&quot;) Wald test for gdppc gdppc_l.democracy in glm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) F = 13.48875 on 2 and 7728 df: p= 1.4194e-06 With \\(1.4194e-06\\) we can reject the null hypothesis, and conclude that jointly, the slope coefficients are different from zero, and as such that per capita GDP explains democratic survival. Literature Recommendations Brambor et al. (2006): This is for the Joint Estimation of Emergence and Survival This discussion draws on Brambor et al. (2006). Please note that I am deliberately NOT lagging the independent variables (IVs) on this worksheet to keep notation as simple as possible. If you decide to run the interaction model, make sure you lag the IVs as explained above.↩︎ "],["exercises-2.html", "Exercises The Data Set Basics Advanced", " Exercises The Data Set Use the data set called prz.dta which is available in the Downloads Section. This is the data set used in the book “Democracy and Development” (Przeworski et al., 2000). Deposit this in an appropriate working directory and import the data set into a data frame called prz. We will only be looking at a few variables – democ = 1 if democracy, 0 otherwise; gdpw - GDP per worker; g = growth rate; oil = 1 if oil producer, 0 otherwise.10 Basics Run a probit model where democ is the dependent variable and g, gdpw and oil are the independent variables. Put the results in column 1 of Table 1. What is (possibly) wrong with this approach? Interpret the coefficients on one or two of the variables. Run the same probit model as before but now include a lagged dependent variable. To create the lagged dependent variable, call: prz &lt;- prz %&gt;% group_by(country) %&gt;% mutate(l.democ = lag(democ)) %&gt;% ungroup() Put the results in column two of Table 1. What are we assuming by including a lagged dependent variable? Do you think that this is appropriate here? Now estimate a probit “transition to democracy” model i.e. how do growth, wealth and oil affect the probability that a country is a democracy this year given that it was a dictatorship last year. We are also lagging the independent variables by one year. Put the results in column 3 of Table 1. Interpret the sign of the coefficients on each independent variable. Now estimate a probit “survival of democracy” model i.e. how do (lagged) growth, wealth and oil affect the probability that a country is a democracy this year given that it was a democracy last year. Put the results in column 4 of Table 1. Interpret the sign of the coefficients on each independent variable. Advanced This section draws on the instructions for joint estimation. Now interact all the lagged independent variables with the lagged dependent variable. Estimate a fully interactive model and include all the constitutive terms. Put the results in column 5 of Table 1. What is the relationship between these coefficients and those in the previous two columns? Is there any extra information provided by this full interaction model that was not available from the previous two models? Now consider the straight probit model, the probit model with the lagged dependent variable, and the full interaction model. Produce the ROC curve for each of these models. Interpret a point on one of these curves. What do the ROC curves tell you about the fit of these three models? You can find the solutions to these exercises in the respective RScript in the Downloads Section. But you can also download the results table, and the stargazer code to produce this table. I have taken these exercises from some material written by Matt Golder from whom I learned all this many moons ago.↩︎ "],["downloads.html", "Downloads Documents Data Sets (in alphabetical order) R Scripts", " Downloads Documents PO3B3 Bibliography Week 7 Stargazer Table Data Sets (in alphabetical order) Africa Europe Eastern Europe Example Latin America Middle East Przeworski et al. (2000) South East Asia Sub-Saharan Africa World World Value Survey (WVS) R Scripts Week 1 Week 3 Week 3 Predicted Probability Graph Week 3 Sample Stargazer Code Week 5 Solutions Week 7 Solutions Week 7 Stargazer Table "],["list-of-references.html", "List of References", " List of References Boix, C., Miller, M., &amp; Rosato, S. (2018). Boix-Miller-Rosato Dichotomous Coding of Democracy, 1800-2015 (Version V3). Harvard Dataverse. https://doi.org/10.7910/DVN/FJLMKT Boix, C., &amp; Stokes, S. (2003). Endogenous Democratization. World Politics, 55, 517–549. Brambor, T., Clark, W. R., &amp; Golder, M. (2006). Understanding Interaction Models: Improving Empirical Analysis. Political Analysis, 14, 62–83. Diamond, L. (1992). Economic Development and Democracy Reconsidered. American Behavioral Scientist, 35(4/5), 450–499. Epstein, D. L., Bates, R., Goldstone, J., Kristensen, I., &amp; O’Halloran, S. (2006). Democratic Transitions. American Journal of Political Science, 50(3), 551–569. Fogarty, B. J. (2023). Quantitative Social Science Data With R (Second). Thousand Oaks, CA: Sage. King, G. (1995). Replication, replication. PS: Political Science and Politics, 28(3), 541–559. Lipset, S. M. (1959). Some Social Requisites of Democracy: Economic Development and Political Legitimacy. The American Political Science Review, 53(1), 69–105. Long, J. D., &amp; Teetor, P. (2019). R cookbook: proven recipes for data analysis, statistics, and graphics. O’Reilly Media. Long, J. S. (1997). Regression Models for Categorial and Limited Dependent Variables. Thousand Oaks: Sage. Marshall, M. G., &amp; Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and Transitions, 1800-2018. available online at https://www.systemicpeace.org/inscr/p5manualv2018.pdf. Przeworski, A., Alvarez, M. E., Cheibub, J. A., &amp; Limongi, F. (2000). Democracy and Development - Political Institutions and Well-Being in the World, 1950-1990. Cambridge: Cambridge University Press. Reiche, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press. Stinerock, R. (2022). Statistics with R – A Beginner’s Guide (Second). Thousand Oaks, CA: Sage. World Bank. (2024). World Development Indicators, 21.02.2024. available online at https://datacatalog.worldbank.org/dataset/world-development-indicators. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
